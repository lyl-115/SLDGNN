{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9888e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os   \n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import precision_score,recall_score\n",
    "# from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.datasets import CitationFull\n",
    "from torch_geometric.datasets import Coauthor\n",
    "from torch_geometric.datasets import Amazon\n",
    "from torch_geometric.datasets import Actor,WebKB,NELL,Reddit,AttributedGraphDataset,HeterophilousGraphDataset,Planetoid\n",
    "from torch_geometric.nn.models import GAT,GIN\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch_geometric.transforms as T\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import Linear,GCNConv,SAGEConv,GATConv,GATv2Conv,ChebConv,SGConv,FAConv,EGConv,GCN2Conv,GeneralConv,SSGConv\n",
    "from torch_geometric.utils import degree\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b8aed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: CoauthorPhysics():\n",
      "Number of graphs: 1\n",
      "Number of features: 8415\n",
      "Number of classes: 5\n",
      "torch.Size([34493, 8415])\n",
      "torch.Size([2, 495924])\n",
      "Avg degree: 14.37752628326416\n"
     ]
    }
   ],
   "source": [
    "dsname=\"Physics\"\n",
    "\n",
    "if dsname in [\"CS\",\"Physics\",\"BlogCatalog\",\"WIKI\",\"cora_ml\",\"citeseer\",\"pubmed\",\"Photo\",\"Computers\" ]:\n",
    "    if dsname ==\"CS\":\n",
    "        nm = Coauthor(root=\"./datasets2\",name=dsname)\n",
    "    if dsname ==\"Physics\":\n",
    "        nm = Coauthor(root=\"./datasets2\",name=dsname)\n",
    "    if dsname ==\"BlogCatalog\":\n",
    "        nm=AttributedGraphDataset(root=\"./datasets\",name=dsname)\n",
    "    if dsname ==\"WIKI\":\n",
    "        nm=AttributedGraphDataset(root=\"./datasets\",name=dsname)\n",
    "    if dsname ==\"cora_ml\":\n",
    "        nm = CitationFull(root=\"./datasets\",name=dsname)\n",
    "    if dsname ==\"citeseer\":\n",
    "        nm = CitationFull(root=\"./datasets\",name=dsname)\n",
    "    if dsname ==\"pubmed\":\n",
    "        nm = CitationFull(root=\"./datasets\",name=dsname)\n",
    "    if dsname ==\"Photo\":\n",
    "        nm = Amazon(root=\"./datasets\",name=dsname)\n",
    "    if dsname ==\"Computers\":\n",
    "        nm = Amazon(root=\"./datasets\",name=dsname)\n",
    "        \n",
    "    print(f'Dataset: {nm}:')\n",
    "    print(f'Number of graphs: {len(nm)}')\n",
    "    print(f'Number of features: {nm.num_features}')\n",
    "    print(f'Number of classes: {nm.num_classes}')\n",
    "    print(nm.x.shape)\n",
    "    print(nm.edge_index.shape)\n",
    "    node_degree=degree(nm[0].edge_index[0])\n",
    "    avgdegree=node_degree.mean().item()\n",
    "    print(f'Avg degree: {avgdegree}')\n",
    "else:\n",
    "    print(\"The name of datsets error,try again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd18936e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f35ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def khop_graphs_sparse(x, edge_index, k,name,device,features=True, regular=False):\n",
    "    # Comprobamos si ya existe el fichero\n",
    "    if os.path.isfile('./data/hops_'+name+'.pkl'):\n",
    "        import pickle\n",
    "        with open('./data/hops_'+name+'.pkl', 'rb') as f:\n",
    "            hops = pickle.load(f)\n",
    "        return hops\n",
    "    similarity = torch.cdist(x, x, p=2)\n",
    "    # Normalize between 0 and 1\n",
    "    similarity = (similarity - similarity.min()) / (similarity.max() - similarity.min())\n",
    "    similarity = similarity.to(device)\n",
    "    hops = list()\n",
    "    attributes = list()\n",
    "    N = edge_index.max().item() + 1\n",
    "    # Create the adjacency matrix\n",
    "    A = torch.sparse_coo_tensor(edge_index, torch.ones(edge_index.size(1)), (N, N)).to(device)\n",
    "    # Add self loops\n",
    "    I = torch.sparse_coo_tensor(torch.arange(N).unsqueeze(0).repeat(2, 1), torch.ones(N), (N, N)).to(device)\n",
    "    A = A + I\n",
    "    # Degree matrix\n",
    "    degrees = torch.sparse.sum(A, dim=1)\n",
    "    degrees = torch.pow(degrees, -0.5)\n",
    "    # Get the indices of the diagonal elements\n",
    "    indices = torch.arange(N).unsqueeze(0).repeat(2, 1).to(device)\n",
    "    values = degrees.coalesce().values().to(device)\n",
    "    # Create the sparse diagonal matrix\n",
    "    D_tilde = torch.sparse_coo_tensor(indices, values, (N, N)).to(device)\n",
    "    A_tilde = torch.sparse.mm(torch.sparse.mm(D_tilde, A), D_tilde)\n",
    "    # Compute A_tilde^k\n",
    "    A_tilde_k = A_tilde.clone().to(device)\n",
    "    hops.append(A_tilde_k.clone().coalesce().indices().to(device))\n",
    "    # Ahora ponemos los pesos de cada una de las aristas\n",
    "    #attributes.append(A_tilde_k.clone().coalesce().values().to(device))\n",
    "    for i in range(k - 1):\n",
    "        print(\"Computing k: \", i+1, \" of \", k-1)\n",
    "        if device == 'cpu':\n",
    "            # Mostramos cuanta memoria ram del sistema estamos usando\n",
    "            print(\"Ram memory: \", psutil.virtual_memory().percent, \"%\")\n",
    "        else:\n",
    "            # Mostramos cuanta memoria estamos usando\n",
    "            print(torch.cuda.memory_allocated(device=device), \"out of \", torch.cuda.max_memory_allocated(device=device))\n",
    "        A_tilde_k = torch.sparse.mm(A_tilde_k, A_tilde)\n",
    "        # We store those indices that in similarity has a value greater than 0.5\n",
    "        print(\"Before pruning: \", A_tilde_k.coalesce().indices().size(1))\n",
    "        if features:\n",
    "            indices = A_tilde_k.coalesce().indices().to(device)\n",
    "            indices = indices[:, similarity[indices[0], indices[1]] >= similarity.mean()]\n",
    "            # Select only the initial number of edges\n",
    "            if regular == False:\n",
    "                if indices.size(1) > edge_index.size(1):\n",
    "                    # We select the edges with the highest similarity\n",
    "                    indices = indices[:, similarity[indices[0], indices[1]].argsort(descending=True)[:edge_index.size(1)]]       \n",
    "            print(\"After pruning: \", indices.size(1))\n",
    "            hops.append(indices.clone())\n",
    "\n",
    "            #A_tilde_k = torch.sparse_coo_tensor(indices, torch.ones(indices.size(1)), (N, N)).to(device)\n",
    "    #    attributes.append(A_tilde_k.clone().coalesce().values().to(device))\n",
    "    # Nos guardamos la lista de hops\n",
    "    import pickle\n",
    "    with open('./data/hops_'+name+'.pkl', 'wb') as f:\n",
    "        pickle.dump(hops, f)\n",
    "    return hops#, attributes        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3470643b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "init_edge_index = nm.edge_index.clone()\n",
    "hops = khop_graphs_sparse(nm.x,nm.edge_index,3,nm.name,\"cpu\")\n",
    "hops.append(init_edge_index)\n",
    "print(\"Done!\")\n",
    "nm.edge_index = hops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b380e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc1400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MO_GNN_large(torch.nn.Module):\n",
    "    def __init__(self, in_channels,hidden_channels, out_channels,num_layers,dropout=0.2,seed=12345):\n",
    "        super(MO_GNN_large, self).__init__()\n",
    "        # seed\n",
    "        torch.manual_seed(seed)\n",
    "        # Create the layers\n",
    "        self.MLP = torch.nn.Linear(in_channels,hidden_channels)\n",
    "        self.MLP2 = torch.nn.Linear(hidden_channels,hidden_channels)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.init_conv = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn_extra = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.init_conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn_extra_2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        self.bn = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(GCNConv(in_channels, hidden_channels, cached=False, normalize=True, add_self_loops=True))\n",
    "            self.bn.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        # Final layer\n",
    "        #self.fc1 = Linear((num_layers + 2)*hidden_channels, out_channels)\n",
    "        self.fc1 = Linear(hidden_channels, out_channels)\n",
    "        # Attention mechanism\n",
    "        self.att = nn.Parameter(torch.ones(num_layers + 2))\n",
    "        self.sm = nn.Softmax(dim=0)\n",
    "        # Dropout\n",
    "        self.dropout = dropout\n",
    "    def forward(self, x, edge_indexes):\n",
    "        mask = self.sm(self.att)\n",
    "#         print(mask)\n",
    "#         print(mask.shape)\n",
    "#         print(mask[-1])\n",
    "        \n",
    "#         print(edge_indexes[0])\n",
    "#         print(edge_indexes[-1])\n",
    "        # GCNConv over the original graph\n",
    "#         print(edge_indexes[-1])\n",
    "#         print(edge_indexes[-1]).shape\n",
    "        extra_conv = self.init_conv(x, edge_indexes[-1]).relu()\n",
    "        extra_conv = F.dropout(extra_conv, p=0.5, training=self.training)\n",
    "        extra_conv = self.init_conv2(extra_conv, edge_indexes[-1]).relu() * mask[-1]\n",
    "        \n",
    "        # GCNConv over the n hops of graph\n",
    "        embeddings = list()\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            tmp_embedding = conv(x, edge_indexes[i]).relu() * mask[i]\n",
    "            embeddings.append(tmp_embedding.unsqueeze(0))\n",
    "        # MLP over the features of the graph\n",
    "        x = self.MLP(x).relu()\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.MLP2(x).relu() * mask[-2]\n",
    "        # Sum all the embeddings\n",
    "        final_embedding = torch.cat(embeddings,dim=0)\n",
    "        final_embedding = torch.cat([final_embedding, x.unsqueeze(0)], dim=0)\n",
    "        final_embedding = torch.cat([final_embedding, extra_conv.unsqueeze(0)], dim=0)\n",
    "        # Sum all the embeddings\n",
    "        final_embedding = final_embedding.sum(dim=0)\n",
    "        z = F.dropout(final_embedding, p=self.dropout, training=self.training)\n",
    "        z = self.fc1(z).log_softmax(dim=-1)\n",
    "        return z\n",
    "\n",
    "\n",
    "def ind2mask(a, n):\n",
    "    b = np.zeros(n)\n",
    "    b[a] = 1\n",
    "    return torch.tensor(b)\n",
    "\n",
    "\n",
    "def train(model,data,edge_gpu,train_set_ind,train_label_ind):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "#     print(\"nininini:::{}\".format(data.edge_index))\n",
    "    out = model(data.x,edge_gpu)\n",
    "    label=torch.nn.functional.one_hot(data.y[train_label_ind], num_classes=nm.num_classes)\n",
    "    label=label.to(torch.float32)    \n",
    "    train_set_ind=train_set_ind.to(dtype=torch.int64)\n",
    "\n",
    "    loss = F.cross_entropy(out[train_label_ind], label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,data,edge_gpu,train_set_ind,train_label_ind,test_label_ind):\n",
    "    model.eval()\n",
    "    ret = model(data.x, edge_gpu)\n",
    "    pred=ret.argmax(dim=-1)\n",
    "    \n",
    "    mask = train_label_ind\n",
    "    trainacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "    mask = test_label_ind\n",
    "    testacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "        \n",
    "    return trainacc,testacc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     dataset = CitationFull(\"./datasets\", name=\"citeseer\", transform=T.NormalizeFeatures())\n",
    "    data = nm[0].to(device)\n",
    "    edge_gpu = [tensor.to(device) for tensor in nm.edge_index]\n",
    "    \n",
    "\n",
    "    framedata=[]\n",
    "    for percent in range(1,10):\n",
    "        for num in range(10):\n",
    "\n",
    "            index=[i for i in range(nm.x.shape[0])]\n",
    "            train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1*percent,random_state=42,stratify=nm[0].y)\n",
    "            train_mask=ind2mask(train_set_ind, nm.x.shape[0])\n",
    "            test_mask=ind2mask(test_set_ind, nm.x.shape[0])\n",
    "\n",
    "            value=[0.1*percent,0,0,0,0]\n",
    "            \n",
    "            \n",
    "            model = MO_GNN_large(nm.num_features,32,nm.num_classes,2).to(device)\n",
    "#             model = torch_geometric.compile(model)                   # Compile the model into an optimized version:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "            \n",
    "            for epoch in range(0, 400):\n",
    "                loss = train(model,data,edge_gpu,train_mask,train_label_ind)\n",
    "                train_acc,test_acc = test(model,data,edge_gpu,test_mask,train_label_ind,test_label_ind)\n",
    "                print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train: {train_acc:.4f},'f'Test: {test_acc:.4f}')\n",
    "                if test_acc > value[4]:\n",
    "                    value[4] = test_acc \n",
    "                    value[1] = epoch\n",
    "                    value[2] = loss\n",
    "                    value[3] = train_acc\n",
    "            print(value)\n",
    "            framedata.append(value)\n",
    "            time.sleep(0.5)\n",
    "    pd.DataFrame(framedata).to_csv(\"./HEXGNN/HEXGNN_{}_3hop.csv\".format(dsname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef8169a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
