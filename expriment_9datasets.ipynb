{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed2c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score,recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.datasets import CitationFull\n",
    "from torch_geometric.datasets import Coauthor\n",
    "from torch_geometric.datasets import Amazon\n",
    "from torch_geometric.datasets import Actor,AttributedGraphDataset,HeterophilousGraphDataset,Planetoid,WikipediaNetwork,Coauthor\n",
    "from torch_geometric.nn import GAT,GIN\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch_geometric.transforms as T\n",
    "import torch_geometric\n",
    "import torch\n",
    "from torch_geometric.nn import Linear,GCNConv,SuperGATConv,GATConv,GeneralConv,SGConv,FAConv,EGConv,GCN2Conv,SSGConv,GENConv\n",
    "from torch_geometric.utils import degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e794bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: CoauthorPhysics():\n",
      "Number of graphs: 1\n",
      "Number of features: 8415\n",
      "Number of classes: 5\n",
      "torch.Size([34493, 8415])\n",
      "torch.Size([2, 495924])\n",
      "Avg degree: 14.37752628326416\n"
     ]
    }
   ],
   "source": [
    "dsname=\"Physics\"\n",
    "\n",
    "if dsname in [\"CS\",\"Physics\",\"BlogCatalog\",\"WIKI\",\"cora_ml\",\"citeseer\",\"pubmed\",\"Photo\",\"Computers\" ]:\n",
    "    if dsname ==\"CS\":\n",
    "        nm = Coauthor(root=\"./datasets2\",name=dsname)\n",
    "    if dsname ==\"Physics\":\n",
    "        nm = Coauthor(root=\"./datasets2\",name=dsname)\n",
    "    if dsname ==\"BlogCatalog\":\n",
    "        nm=AttributedGraphDataset(root=\"./datasets\",name=dsname)\n",
    "    if dsname ==\"WIKI\":\n",
    "        nm=AttributedGraphDataset(root=\"./datasets\",name=dsname)\n",
    "    if dsname ==\"cora_ml\":\n",
    "        nm = CitationFull(root=\"./datasets\",name=dsname)\n",
    "    if dsname ==\"citeseer\":\n",
    "        nm = CitationFull(root=\"./datasets\",name=dsname)\n",
    "    if dsname ==\"pubmed\":\n",
    "        nm = CitationFull(root=\"./datasets\",name=dsname)\n",
    "    if dsname ==\"Photo\":\n",
    "        nm = Amazon(root=\"./datasets\",name=dsname)\n",
    "    if dsname ==\"Computers\":\n",
    "        nm = Amazon(root=\"./datasets\",name=dsname)\n",
    "        \n",
    "    print(f'Dataset: {nm}:')\n",
    "    print(f'Number of graphs: {len(nm)}')\n",
    "    print(f'Number of features: {nm.num_features}')\n",
    "    print(f'Number of classes: {nm.num_classes}')\n",
    "    print(nm.x.shape)\n",
    "    print(nm.edge_index.shape)\n",
    "    node_degree=degree(nm[0].edge_index[0])\n",
    "    avgdegree=node_degree.mean().item()\n",
    "    print(f'Avg degree: {avgdegree}')\n",
    "    \n",
    "else:\n",
    "    print(\"The name of datsets error,try again!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681e397",
   "metadata": {},
   "source": [
    "### GCN_2L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfdc4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 17:00:53.656109: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-05 17:00:54.657862: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-05 17:00:57.158473: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/root/anaconda3/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:90: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.6084, Train: 0.6510,Test: 0.6490\n",
      "Epoch: 1, Loss: 1.5045, Train: 0.6954,Test: 0.7023\n",
      "Epoch: 2, Loss: 1.3555, Train: 0.7378,Test: 0.7426\n",
      "Epoch: 3, Loss: 1.2434, Train: 0.7991,Test: 0.8006\n",
      "Epoch: 4, Loss: 1.1678, Train: 0.8314,Test: 0.8319\n",
      "Epoch: 5, Loss: 1.1149, Train: 0.8565,Test: 0.8586\n",
      "Epoch: 6, Loss: 1.0808, Train: 0.8826,Test: 0.8829\n",
      "Epoch: 7, Loss: 1.0564, Train: 0.9032,Test: 0.9093\n",
      "Epoch: 8, Loss: 1.0351, Train: 0.9173,Test: 0.9220\n",
      "Epoch: 9, Loss: 1.0162, Train: 0.9280,Test: 0.9296\n",
      "Epoch: 10, Loss: 1.0006, Train: 0.9341,Test: 0.9345\n",
      "Epoch: 11, Loss: 0.9887, Train: 0.9394,Test: 0.9432\n",
      "Epoch: 12, Loss: 0.9801, Train: 0.9449,Test: 0.9467\n",
      "Epoch: 13, Loss: 0.9742, Train: 0.9477,Test: 0.9493\n",
      "Epoch: 14, Loss: 0.9706, Train: 0.9488,Test: 0.9493\n",
      "Epoch: 15, Loss: 0.9685, Train: 0.9495,Test: 0.9496\n",
      "Epoch: 16, Loss: 0.9667, Train: 0.9495,Test: 0.9487\n",
      "Epoch: 17, Loss: 0.9645, Train: 0.9506,Test: 0.9507\n",
      "Epoch: 18, Loss: 0.9618, Train: 0.9520,Test: 0.9522\n",
      "Epoch: 19, Loss: 0.9592, Train: 0.9535,Test: 0.9548\n",
      "Epoch: 20, Loss: 0.9571, Train: 0.9542,Test: 0.9533\n",
      "Epoch: 21, Loss: 0.9555, Train: 0.9545,Test: 0.9548\n",
      "Epoch: 22, Loss: 0.9545, Train: 0.9545,Test: 0.9568\n",
      "Epoch: 23, Loss: 0.9539, Train: 0.9544,Test: 0.9583\n",
      "Epoch: 24, Loss: 0.9535, Train: 0.9549,Test: 0.9588\n",
      "Epoch: 25, Loss: 0.9530, Train: 0.9551,Test: 0.9600\n",
      "Epoch: 26, Loss: 0.9524, Train: 0.9557,Test: 0.9603\n",
      "Epoch: 27, Loss: 0.9518, Train: 0.9566,Test: 0.9609\n",
      "Epoch: 28, Loss: 0.9511, Train: 0.9576,Test: 0.9617\n",
      "Epoch: 29, Loss: 0.9505, Train: 0.9579,Test: 0.9612\n",
      "Epoch: 30, Loss: 0.9500, Train: 0.9587,Test: 0.9612\n",
      "Epoch: 31, Loss: 0.9497, Train: 0.9587,Test: 0.9603\n",
      "Epoch: 32, Loss: 0.9494, Train: 0.9592,Test: 0.9603\n",
      "Epoch: 33, Loss: 0.9492, Train: 0.9594,Test: 0.9600\n",
      "Epoch: 34, Loss: 0.9490, Train: 0.9597,Test: 0.9600\n",
      "Epoch: 35, Loss: 0.9489, Train: 0.9599,Test: 0.9612\n",
      "Epoch: 36, Loss: 0.9487, Train: 0.9603,Test: 0.9614\n",
      "Epoch: 37, Loss: 0.9485, Train: 0.9608,Test: 0.9614\n",
      "Epoch: 38, Loss: 0.9483, Train: 0.9609,Test: 0.9626\n",
      "Epoch: 39, Loss: 0.9481, Train: 0.9611,Test: 0.9626\n",
      "Epoch: 40, Loss: 0.9480, Train: 0.9612,Test: 0.9629\n",
      "Epoch: 41, Loss: 0.9480, Train: 0.9612,Test: 0.9629\n",
      "Epoch: 42, Loss: 0.9479, Train: 0.9615,Test: 0.9632\n",
      "Epoch: 43, Loss: 0.9478, Train: 0.9617,Test: 0.9632\n",
      "Epoch: 44, Loss: 0.9477, Train: 0.9620,Test: 0.9629\n",
      "Epoch: 45, Loss: 0.9476, Train: 0.9622,Test: 0.9629\n",
      "Epoch: 46, Loss: 0.9475, Train: 0.9626,Test: 0.9620\n",
      "Epoch: 47, Loss: 0.9474, Train: 0.9627,Test: 0.9614\n",
      "Epoch: 48, Loss: 0.9474, Train: 0.9629,Test: 0.9612\n",
      "Epoch: 49, Loss: 0.9473, Train: 0.9631,Test: 0.9614\n",
      "Epoch: 50, Loss: 0.9473, Train: 0.9632,Test: 0.9614\n",
      "Epoch: 51, Loss: 0.9472, Train: 0.9635,Test: 0.9617\n",
      "Epoch: 52, Loss: 0.9472, Train: 0.9638,Test: 0.9620\n",
      "Epoch: 53, Loss: 0.9471, Train: 0.9638,Test: 0.9629\n",
      "Epoch: 54, Loss: 0.9470, Train: 0.9640,Test: 0.9626\n",
      "Epoch: 55, Loss: 0.9470, Train: 0.9641,Test: 0.9632\n",
      "Epoch: 56, Loss: 0.9469, Train: 0.9645,Test: 0.9629\n",
      "Epoch: 57, Loss: 0.9468, Train: 0.9646,Test: 0.9626\n",
      "Epoch: 58, Loss: 0.9466, Train: 0.9650,Test: 0.9629\n",
      "Epoch: 59, Loss: 0.9465, Train: 0.9652,Test: 0.9626\n",
      "Epoch: 60, Loss: 0.9463, Train: 0.9653,Test: 0.9629\n",
      "Epoch: 61, Loss: 0.9463, Train: 0.9654,Test: 0.9626\n",
      "Epoch: 62, Loss: 0.9462, Train: 0.9657,Test: 0.9623\n",
      "Epoch: 63, Loss: 0.9460, Train: 0.9659,Test: 0.9623\n",
      "Epoch: 64, Loss: 0.9459, Train: 0.9659,Test: 0.9632\n",
      "Epoch: 65, Loss: 0.9458, Train: 0.9661,Test: 0.9629\n",
      "Epoch: 66, Loss: 0.9457, Train: 0.9663,Test: 0.9629\n",
      "Epoch: 67, Loss: 0.9456, Train: 0.9664,Test: 0.9635\n",
      "Epoch: 68, Loss: 0.9455, Train: 0.9665,Test: 0.9632\n",
      "Epoch: 69, Loss: 0.9454, Train: 0.9666,Test: 0.9629\n",
      "Epoch: 70, Loss: 0.9453, Train: 0.9668,Test: 0.9629\n",
      "Epoch: 71, Loss: 0.9451, Train: 0.9669,Test: 0.9632\n",
      "Epoch: 72, Loss: 0.9450, Train: 0.9669,Test: 0.9635\n",
      "Epoch: 73, Loss: 0.9449, Train: 0.9670,Test: 0.9641\n",
      "Epoch: 74, Loss: 0.9448, Train: 0.9671,Test: 0.9641\n",
      "Epoch: 75, Loss: 0.9446, Train: 0.9672,Test: 0.9641\n",
      "Epoch: 76, Loss: 0.9445, Train: 0.9673,Test: 0.9643\n",
      "Epoch: 77, Loss: 0.9444, Train: 0.9674,Test: 0.9646\n",
      "Epoch: 78, Loss: 0.9442, Train: 0.9675,Test: 0.9649\n",
      "Epoch: 79, Loss: 0.9441, Train: 0.9675,Test: 0.9652\n",
      "Epoch: 80, Loss: 0.9440, Train: 0.9677,Test: 0.9655\n",
      "Epoch: 81, Loss: 0.9439, Train: 0.9678,Test: 0.9652\n",
      "Epoch: 82, Loss: 0.9438, Train: 0.9677,Test: 0.9652\n",
      "Epoch: 83, Loss: 0.9436, Train: 0.9678,Test: 0.9655\n",
      "Epoch: 84, Loss: 0.9435, Train: 0.9680,Test: 0.9655\n",
      "Epoch: 85, Loss: 0.9434, Train: 0.9680,Test: 0.9658\n",
      "Epoch: 86, Loss: 0.9433, Train: 0.9681,Test: 0.9658\n",
      "Epoch: 87, Loss: 0.9432, Train: 0.9684,Test: 0.9658\n",
      "Epoch: 88, Loss: 0.9430, Train: 0.9685,Test: 0.9655\n",
      "Epoch: 89, Loss: 0.9429, Train: 0.9685,Test: 0.9658\n",
      "Epoch: 90, Loss: 0.9428, Train: 0.9688,Test: 0.9655\n",
      "Epoch: 91, Loss: 0.9427, Train: 0.9688,Test: 0.9655\n",
      "Epoch: 92, Loss: 0.9426, Train: 0.9689,Test: 0.9655\n",
      "Epoch: 93, Loss: 0.9425, Train: 0.9690,Test: 0.9655\n",
      "Epoch: 94, Loss: 0.9424, Train: 0.9691,Test: 0.9655\n",
      "Epoch: 95, Loss: 0.9423, Train: 0.9693,Test: 0.9655\n",
      "Epoch: 96, Loss: 0.9422, Train: 0.9693,Test: 0.9652\n",
      "Epoch: 97, Loss: 0.9421, Train: 0.9695,Test: 0.9652\n",
      "Epoch: 98, Loss: 0.9420, Train: 0.9696,Test: 0.9652\n",
      "Epoch: 99, Loss: 0.9419, Train: 0.9698,Test: 0.9652\n",
      "[0.1, 85, 0.9433974027633667, 0.9680443256128596, 0.9657971014492753]\n",
      "Epoch: 0, Loss: 1.6113, Train: 0.7824,Test: 0.7809\n",
      "Epoch: 1, Loss: 1.5593, Train: 0.7679,Test: 0.7728\n",
      "Epoch: 2, Loss: 1.4377, Train: 0.7849,Test: 0.7913\n",
      "Epoch: 3, Loss: 1.3061, Train: 0.8016,Test: 0.8055\n",
      "Epoch: 4, Loss: 1.2075, Train: 0.8177,Test: 0.8235\n",
      "Epoch: 5, Loss: 1.1450, Train: 0.8373,Test: 0.8441\n",
      "Epoch: 6, Loss: 1.1051, Train: 0.8554,Test: 0.8603\n",
      "Epoch: 7, Loss: 1.0783, Train: 0.8731,Test: 0.8751\n",
      "Epoch: 8, Loss: 1.0580, Train: 0.8912,Test: 0.8951\n",
      "Epoch: 9, Loss: 1.0396, Train: 0.9102,Test: 0.9130\n",
      "Epoch: 10, Loss: 1.0217, Train: 0.9238,Test: 0.9252\n",
      "Epoch: 11, Loss: 1.0057, Train: 0.9334,Test: 0.9354\n",
      "Epoch: 12, Loss: 0.9934, Train: 0.9411,Test: 0.9414\n",
      "Epoch: 13, Loss: 0.9847, Train: 0.9464,Test: 0.9481\n",
      "Epoch: 14, Loss: 0.9785, Train: 0.9496,Test: 0.9522\n",
      "Epoch: 15, Loss: 0.9732, Train: 0.9517,Test: 0.9507\n",
      "Epoch: 16, Loss: 0.9689, Train: 0.9516,Test: 0.9519\n",
      "Epoch: 17, Loss: 0.9659, Train: 0.9516,Test: 0.9528\n",
      "Epoch: 18, Loss: 0.9633, Train: 0.9519,Test: 0.9533\n",
      "Epoch: 19, Loss: 0.9609, Train: 0.9528,Test: 0.9542\n",
      "Epoch: 20, Loss: 0.9589, Train: 0.9532,Test: 0.9545\n",
      "Epoch: 21, Loss: 0.9574, Train: 0.9534,Test: 0.9565\n",
      "Epoch: 22, Loss: 0.9565, Train: 0.9532,Test: 0.9562\n",
      "Epoch: 23, Loss: 0.9558, Train: 0.9533,Test: 0.9559\n",
      "Epoch: 24, Loss: 0.9552, Train: 0.9541,Test: 0.9571\n",
      "Epoch: 25, Loss: 0.9545, Train: 0.9546,Test: 0.9583\n",
      "Epoch: 26, Loss: 0.9536, Train: 0.9557,Test: 0.9591\n",
      "Epoch: 27, Loss: 0.9527, Train: 0.9559,Test: 0.9606\n",
      "Epoch: 28, Loss: 0.9518, Train: 0.9568,Test: 0.9609\n",
      "Epoch: 29, Loss: 0.9511, Train: 0.9573,Test: 0.9597\n",
      "Epoch: 30, Loss: 0.9505, Train: 0.9582,Test: 0.9597\n",
      "Epoch: 31, Loss: 0.9502, Train: 0.9585,Test: 0.9588\n",
      "Epoch: 32, Loss: 0.9500, Train: 0.9589,Test: 0.9606\n",
      "Epoch: 33, Loss: 0.9498, Train: 0.9590,Test: 0.9612\n",
      "Epoch: 34, Loss: 0.9496, Train: 0.9596,Test: 0.9609\n",
      "Epoch: 35, Loss: 0.9493, Train: 0.9598,Test: 0.9617\n",
      "Epoch: 36, Loss: 0.9490, Train: 0.9599,Test: 0.9614\n",
      "Epoch: 37, Loss: 0.9487, Train: 0.9603,Test: 0.9614\n",
      "Epoch: 38, Loss: 0.9485, Train: 0.9608,Test: 0.9614\n",
      "Epoch: 39, Loss: 0.9484, Train: 0.9607,Test: 0.9614\n",
      "Epoch: 40, Loss: 0.9483, Train: 0.9608,Test: 0.9617\n",
      "Epoch: 41, Loss: 0.9483, Train: 0.9612,Test: 0.9614\n",
      "Epoch: 42, Loss: 0.9481, Train: 0.9612,Test: 0.9609\n",
      "Epoch: 43, Loss: 0.9480, Train: 0.9616,Test: 0.9609\n",
      "Epoch: 44, Loss: 0.9478, Train: 0.9618,Test: 0.9612\n",
      "Epoch: 45, Loss: 0.9477, Train: 0.9623,Test: 0.9612\n",
      "Epoch: 46, Loss: 0.9476, Train: 0.9626,Test: 0.9614\n",
      "Epoch: 47, Loss: 0.9476, Train: 0.9629,Test: 0.9620\n",
      "Epoch: 48, Loss: 0.9475, Train: 0.9629,Test: 0.9623\n",
      "Epoch: 49, Loss: 0.9474, Train: 0.9633,Test: 0.9623\n",
      "Epoch: 50, Loss: 0.9473, Train: 0.9635,Test: 0.9623\n",
      "Epoch: 51, Loss: 0.9473, Train: 0.9635,Test: 0.9623\n",
      "Epoch: 52, Loss: 0.9472, Train: 0.9636,Test: 0.9626\n",
      "Epoch: 53, Loss: 0.9471, Train: 0.9638,Test: 0.9626\n",
      "Epoch: 54, Loss: 0.9471, Train: 0.9641,Test: 0.9632\n",
      "Epoch: 55, Loss: 0.9470, Train: 0.9644,Test: 0.9632\n",
      "Epoch: 56, Loss: 0.9468, Train: 0.9644,Test: 0.9632\n",
      "Epoch: 57, Loss: 0.9468, Train: 0.9647,Test: 0.9629\n",
      "Epoch: 58, Loss: 0.9467, Train: 0.9647,Test: 0.9623\n",
      "Epoch: 59, Loss: 0.9466, Train: 0.9649,Test: 0.9626\n",
      "Epoch: 60, Loss: 0.9465, Train: 0.9650,Test: 0.9626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61, Loss: 0.9464, Train: 0.9653,Test: 0.9632\n",
      "Epoch: 62, Loss: 0.9463, Train: 0.9653,Test: 0.9635\n",
      "Epoch: 63, Loss: 0.9462, Train: 0.9655,Test: 0.9635\n",
      "Epoch: 64, Loss: 0.9461, Train: 0.9658,Test: 0.9632\n",
      "Epoch: 65, Loss: 0.9460, Train: 0.9659,Test: 0.9632\n",
      "Epoch: 66, Loss: 0.9459, Train: 0.9662,Test: 0.9629\n",
      "Epoch: 67, Loss: 0.9458, Train: 0.9662,Test: 0.9629\n",
      "Epoch: 68, Loss: 0.9457, Train: 0.9662,Test: 0.9629\n",
      "Epoch: 69, Loss: 0.9456, Train: 0.9662,Test: 0.9629\n",
      "Epoch: 70, Loss: 0.9455, Train: 0.9664,Test: 0.9632\n",
      "Epoch: 71, Loss: 0.9454, Train: 0.9666,Test: 0.9632\n",
      "Epoch: 72, Loss: 0.9453, Train: 0.9669,Test: 0.9632\n",
      "Epoch: 73, Loss: 0.9451, Train: 0.9669,Test: 0.9632\n",
      "Epoch: 74, Loss: 0.9450, Train: 0.9671,Test: 0.9635\n",
      "Epoch: 75, Loss: 0.9448, Train: 0.9672,Test: 0.9635\n",
      "Epoch: 76, Loss: 0.9446, Train: 0.9672,Test: 0.9632\n",
      "Epoch: 77, Loss: 0.9445, Train: 0.9674,Test: 0.9632\n",
      "Epoch: 78, Loss: 0.9444, Train: 0.9674,Test: 0.9638\n",
      "Epoch: 79, Loss: 0.9443, Train: 0.9674,Test: 0.9638\n",
      "Epoch: 80, Loss: 0.9442, Train: 0.9675,Test: 0.9641\n",
      "Epoch: 81, Loss: 0.9440, Train: 0.9675,Test: 0.9643\n",
      "Epoch: 82, Loss: 0.9439, Train: 0.9677,Test: 0.9643\n",
      "Epoch: 83, Loss: 0.9438, Train: 0.9678,Test: 0.9643\n",
      "Epoch: 84, Loss: 0.9437, Train: 0.9677,Test: 0.9643\n",
      "Epoch: 85, Loss: 0.9436, Train: 0.9680,Test: 0.9641\n",
      "Epoch: 86, Loss: 0.9435, Train: 0.9681,Test: 0.9643\n",
      "Epoch: 87, Loss: 0.9434, Train: 0.9681,Test: 0.9643\n",
      "Epoch: 88, Loss: 0.9433, Train: 0.9684,Test: 0.9649\n",
      "Epoch: 89, Loss: 0.9432, Train: 0.9684,Test: 0.9649\n",
      "Epoch: 90, Loss: 0.9431, Train: 0.9685,Test: 0.9649\n",
      "Epoch: 91, Loss: 0.9430, Train: 0.9685,Test: 0.9649\n",
      "Epoch: 92, Loss: 0.9429, Train: 0.9687,Test: 0.9649\n",
      "Epoch: 93, Loss: 0.9428, Train: 0.9687,Test: 0.9649\n",
      "Epoch: 94, Loss: 0.9427, Train: 0.9689,Test: 0.9652\n",
      "Epoch: 95, Loss: 0.9426, Train: 0.9688,Test: 0.9652\n",
      "Epoch: 96, Loss: 0.9425, Train: 0.9689,Test: 0.9652\n",
      "Epoch: 97, Loss: 0.9424, Train: 0.9690,Test: 0.9652\n",
      "Epoch: 98, Loss: 0.9423, Train: 0.9692,Test: 0.9652\n",
      "Epoch: 99, Loss: 0.9422, Train: 0.9692,Test: 0.9652\n",
      "[0.1, 94, 0.9426630735397339, 0.9688818735302644, 0.9652173913043478]\n",
      "Epoch: 0, Loss: 1.6081, Train: 0.5081,Test: 0.5096\n",
      "Epoch: 1, Loss: 1.5184, Train: 0.6076,Test: 0.6099\n",
      "Epoch: 2, Loss: 1.3804, Train: 0.6878,Test: 0.6875\n",
      "Epoch: 3, Loss: 1.2703, Train: 0.7425,Test: 0.7438\n",
      "Epoch: 4, Loss: 1.1937, Train: 0.8312,Test: 0.8339\n",
      "Epoch: 5, Loss: 1.1316, Train: 0.8831,Test: 0.8864\n",
      "Epoch: 6, Loss: 1.0836, Train: 0.9052,Test: 0.9104\n",
      "Epoch: 7, Loss: 1.0508, Train: 0.9201,Test: 0.9255\n",
      "Epoch: 8, Loss: 1.0269, Train: 0.9287,Test: 0.9330\n",
      "Epoch: 9, Loss: 1.0079, Train: 0.9367,Test: 0.9397\n",
      "Epoch: 10, Loss: 0.9932, Train: 0.9443,Test: 0.9464\n",
      "Epoch: 11, Loss: 0.9829, Train: 0.9481,Test: 0.9501\n",
      "Epoch: 12, Loss: 0.9763, Train: 0.9503,Test: 0.9472\n",
      "Epoch: 13, Loss: 0.9723, Train: 0.9501,Test: 0.9493\n",
      "Epoch: 14, Loss: 0.9689, Train: 0.9510,Test: 0.9504\n",
      "Epoch: 15, Loss: 0.9653, Train: 0.9521,Test: 0.9507\n",
      "Epoch: 16, Loss: 0.9618, Train: 0.9528,Test: 0.9516\n",
      "Epoch: 17, Loss: 0.9591, Train: 0.9525,Test: 0.9548\n",
      "Epoch: 18, Loss: 0.9573, Train: 0.9526,Test: 0.9542\n",
      "Epoch: 19, Loss: 0.9561, Train: 0.9528,Test: 0.9557\n",
      "Epoch: 20, Loss: 0.9553, Train: 0.9531,Test: 0.9568\n",
      "Epoch: 21, Loss: 0.9545, Train: 0.9535,Test: 0.9568\n",
      "Epoch: 22, Loss: 0.9538, Train: 0.9544,Test: 0.9577\n",
      "Epoch: 23, Loss: 0.9530, Train: 0.9552,Test: 0.9586\n",
      "Epoch: 24, Loss: 0.9521, Train: 0.9562,Test: 0.9594\n",
      "Epoch: 25, Loss: 0.9514, Train: 0.9569,Test: 0.9606\n",
      "Epoch: 26, Loss: 0.9507, Train: 0.9575,Test: 0.9609\n",
      "Epoch: 27, Loss: 0.9502, Train: 0.9582,Test: 0.9594\n",
      "Epoch: 28, Loss: 0.9499, Train: 0.9586,Test: 0.9591\n",
      "Epoch: 29, Loss: 0.9497, Train: 0.9589,Test: 0.9586\n",
      "Epoch: 30, Loss: 0.9494, Train: 0.9592,Test: 0.9588\n",
      "Epoch: 31, Loss: 0.9491, Train: 0.9595,Test: 0.9609\n",
      "Epoch: 32, Loss: 0.9487, Train: 0.9597,Test: 0.9614\n",
      "Epoch: 33, Loss: 0.9484, Train: 0.9599,Test: 0.9626\n",
      "Epoch: 34, Loss: 0.9481, Train: 0.9602,Test: 0.9629\n",
      "Epoch: 35, Loss: 0.9480, Train: 0.9602,Test: 0.9629\n",
      "Epoch: 36, Loss: 0.9479, Train: 0.9602,Test: 0.9629\n",
      "Epoch: 37, Loss: 0.9478, Train: 0.9606,Test: 0.9629\n",
      "Epoch: 38, Loss: 0.9476, Train: 0.9606,Test: 0.9632\n",
      "Epoch: 39, Loss: 0.9475, Train: 0.9609,Test: 0.9629\n",
      "Epoch: 40, Loss: 0.9473, Train: 0.9613,Test: 0.9623\n",
      "Epoch: 41, Loss: 0.9472, Train: 0.9616,Test: 0.9623\n",
      "Epoch: 42, Loss: 0.9471, Train: 0.9619,Test: 0.9617\n",
      "Epoch: 43, Loss: 0.9471, Train: 0.9623,Test: 0.9620\n",
      "Epoch: 44, Loss: 0.9470, Train: 0.9624,Test: 0.9620\n",
      "Epoch: 45, Loss: 0.9469, Train: 0.9623,Test: 0.9620\n",
      "Epoch: 46, Loss: 0.9468, Train: 0.9625,Test: 0.9623\n",
      "Epoch: 47, Loss: 0.9468, Train: 0.9627,Test: 0.9635\n",
      "Epoch: 48, Loss: 0.9467, Train: 0.9630,Test: 0.9638\n",
      "Epoch: 49, Loss: 0.9466, Train: 0.9631,Test: 0.9632\n",
      "Epoch: 50, Loss: 0.9466, Train: 0.9633,Test: 0.9635\n",
      "Epoch: 51, Loss: 0.9465, Train: 0.9635,Test: 0.9635\n",
      "Epoch: 52, Loss: 0.9464, Train: 0.9637,Test: 0.9632\n",
      "Epoch: 53, Loss: 0.9463, Train: 0.9641,Test: 0.9632\n",
      "Epoch: 54, Loss: 0.9462, Train: 0.9642,Test: 0.9629\n",
      "Epoch: 55, Loss: 0.9461, Train: 0.9645,Test: 0.9638\n",
      "Epoch: 56, Loss: 0.9460, Train: 0.9647,Test: 0.9638\n",
      "Epoch: 57, Loss: 0.9459, Train: 0.9650,Test: 0.9635\n",
      "Epoch: 58, Loss: 0.9458, Train: 0.9651,Test: 0.9638\n",
      "Epoch: 59, Loss: 0.9457, Train: 0.9652,Test: 0.9635\n",
      "Epoch: 60, Loss: 0.9456, Train: 0.9654,Test: 0.9629\n",
      "Epoch: 61, Loss: 0.9455, Train: 0.9655,Test: 0.9632\n",
      "Epoch: 62, Loss: 0.9454, Train: 0.9656,Test: 0.9629\n",
      "Epoch: 63, Loss: 0.9453, Train: 0.9659,Test: 0.9632\n",
      "Epoch: 64, Loss: 0.9452, Train: 0.9661,Test: 0.9635\n",
      "Epoch: 65, Loss: 0.9451, Train: 0.9664,Test: 0.9632\n",
      "Epoch: 66, Loss: 0.9450, Train: 0.9664,Test: 0.9632\n",
      "Epoch: 67, Loss: 0.9449, Train: 0.9666,Test: 0.9635\n",
      "Epoch: 68, Loss: 0.9448, Train: 0.9665,Test: 0.9638\n",
      "Epoch: 69, Loss: 0.9447, Train: 0.9667,Test: 0.9643\n",
      "Epoch: 70, Loss: 0.9446, Train: 0.9668,Test: 0.9641\n",
      "Epoch: 71, Loss: 0.9445, Train: 0.9670,Test: 0.9643\n",
      "Epoch: 72, Loss: 0.9443, Train: 0.9670,Test: 0.9646\n",
      "Epoch: 73, Loss: 0.9442, Train: 0.9673,Test: 0.9646\n",
      "Epoch: 74, Loss: 0.9441, Train: 0.9674,Test: 0.9646\n",
      "Epoch: 75, Loss: 0.9440, Train: 0.9676,Test: 0.9646\n",
      "Epoch: 76, Loss: 0.9439, Train: 0.9676,Test: 0.9649\n",
      "Epoch: 77, Loss: 0.9437, Train: 0.9677,Test: 0.9649\n",
      "Epoch: 78, Loss: 0.9436, Train: 0.9678,Test: 0.9649\n",
      "Epoch: 79, Loss: 0.9435, Train: 0.9682,Test: 0.9652\n",
      "Epoch: 80, Loss: 0.9434, Train: 0.9684,Test: 0.9652\n",
      "Epoch: 81, Loss: 0.9433, Train: 0.9685,Test: 0.9652\n",
      "Epoch: 82, Loss: 0.9432, Train: 0.9686,Test: 0.9652\n",
      "Epoch: 83, Loss: 0.9430, Train: 0.9686,Test: 0.9652\n",
      "Epoch: 84, Loss: 0.9429, Train: 0.9687,Test: 0.9655\n",
      "Epoch: 85, Loss: 0.9428, Train: 0.9687,Test: 0.9652\n",
      "Epoch: 86, Loss: 0.9427, Train: 0.9689,Test: 0.9649\n",
      "Epoch: 87, Loss: 0.9426, Train: 0.9690,Test: 0.9649\n",
      "Epoch: 88, Loss: 0.9425, Train: 0.9691,Test: 0.9652\n",
      "Epoch: 89, Loss: 0.9423, Train: 0.9693,Test: 0.9658\n",
      "Epoch: 90, Loss: 0.9422, Train: 0.9694,Test: 0.9658\n",
      "Epoch: 91, Loss: 0.9421, Train: 0.9695,Test: 0.9661\n",
      "Epoch: 92, Loss: 0.9420, Train: 0.9695,Test: 0.9658\n",
      "Epoch: 93, Loss: 0.9419, Train: 0.9695,Test: 0.9658\n",
      "Epoch: 94, Loss: 0.9418, Train: 0.9697,Test: 0.9664\n",
      "Epoch: 95, Loss: 0.9417, Train: 0.9699,Test: 0.9664\n",
      "Epoch: 96, Loss: 0.9415, Train: 0.9699,Test: 0.9664\n",
      "Epoch: 97, Loss: 0.9414, Train: 0.9701,Test: 0.9661\n",
      "Epoch: 98, Loss: 0.9413, Train: 0.9701,Test: 0.9661\n",
      "Epoch: 99, Loss: 0.9412, Train: 0.9703,Test: 0.9661\n",
      "[0.1, 94, 0.9417740702629089, 0.9697194214476693, 0.9663768115942029]\n",
      "Epoch: 0, Loss: 1.6104, Train: 0.7302,Test: 0.7284\n",
      "Epoch: 1, Loss: 1.5284, Train: 0.7179,Test: 0.7171\n",
      "Epoch: 2, Loss: 1.3647, Train: 0.7375,Test: 0.7377\n",
      "Epoch: 3, Loss: 1.2380, Train: 0.7900,Test: 0.7951\n",
      "Epoch: 4, Loss: 1.1581, Train: 0.8381,Test: 0.8435\n",
      "Epoch: 5, Loss: 1.1043, Train: 0.8676,Test: 0.8716\n",
      "Epoch: 6, Loss: 1.0692, Train: 0.8851,Test: 0.8890\n",
      "Epoch: 7, Loss: 1.0471, Train: 0.8971,Test: 0.8968\n",
      "Epoch: 8, Loss: 1.0316, Train: 0.9084,Test: 0.9099\n",
      "Epoch: 9, Loss: 1.0184, Train: 0.9190,Test: 0.9217\n",
      "Epoch: 10, Loss: 1.0061, Train: 0.9284,Test: 0.9293\n",
      "Epoch: 11, Loss: 0.9950, Train: 0.9340,Test: 0.9359\n",
      "Epoch: 12, Loss: 0.9858, Train: 0.9391,Test: 0.9409\n",
      "Epoch: 13, Loss: 0.9783, Train: 0.9425,Test: 0.9426\n",
      "Epoch: 14, Loss: 0.9721, Train: 0.9472,Test: 0.9525\n",
      "Epoch: 15, Loss: 0.9673, Train: 0.9501,Test: 0.9525\n",
      "Epoch: 16, Loss: 0.9646, Train: 0.9502,Test: 0.9496\n",
      "Epoch: 17, Loss: 0.9642, Train: 0.9484,Test: 0.9484\n",
      "Epoch: 18, Loss: 0.9643, Train: 0.9487,Test: 0.9487\n",
      "Epoch: 19, Loss: 0.9630, Train: 0.9506,Test: 0.9507\n",
      "Epoch: 20, Loss: 0.9604, Train: 0.9529,Test: 0.9525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Loss: 0.9576, Train: 0.9544,Test: 0.9551\n",
      "Epoch: 22, Loss: 0.9555, Train: 0.9549,Test: 0.9571\n",
      "Epoch: 23, Loss: 0.9544, Train: 0.9549,Test: 0.9591\n",
      "Epoch: 24, Loss: 0.9539, Train: 0.9550,Test: 0.9600\n",
      "Epoch: 25, Loss: 0.9536, Train: 0.9551,Test: 0.9594\n",
      "Epoch: 26, Loss: 0.9532, Train: 0.9559,Test: 0.9603\n",
      "Epoch: 27, Loss: 0.9526, Train: 0.9565,Test: 0.9600\n",
      "Epoch: 28, Loss: 0.9519, Train: 0.9576,Test: 0.9612\n",
      "Epoch: 29, Loss: 0.9511, Train: 0.9580,Test: 0.9612\n",
      "Epoch: 30, Loss: 0.9504, Train: 0.9582,Test: 0.9620\n",
      "Epoch: 31, Loss: 0.9499, Train: 0.9584,Test: 0.9614\n",
      "Epoch: 32, Loss: 0.9496, Train: 0.9589,Test: 0.9612\n",
      "Epoch: 33, Loss: 0.9494, Train: 0.9594,Test: 0.9600\n",
      "Epoch: 34, Loss: 0.9493, Train: 0.9596,Test: 0.9597\n",
      "Epoch: 35, Loss: 0.9492, Train: 0.9596,Test: 0.9591\n",
      "Epoch: 36, Loss: 0.9491, Train: 0.9601,Test: 0.9588\n",
      "Epoch: 37, Loss: 0.9489, Train: 0.9605,Test: 0.9594\n",
      "Epoch: 38, Loss: 0.9487, Train: 0.9610,Test: 0.9603\n",
      "Epoch: 39, Loss: 0.9485, Train: 0.9614,Test: 0.9603\n",
      "Epoch: 40, Loss: 0.9483, Train: 0.9616,Test: 0.9620\n",
      "Epoch: 41, Loss: 0.9482, Train: 0.9617,Test: 0.9623\n",
      "Epoch: 42, Loss: 0.9480, Train: 0.9615,Test: 0.9626\n",
      "Epoch: 43, Loss: 0.9480, Train: 0.9617,Test: 0.9629\n",
      "Epoch: 44, Loss: 0.9479, Train: 0.9618,Test: 0.9635\n",
      "Epoch: 45, Loss: 0.9479, Train: 0.9622,Test: 0.9635\n",
      "Epoch: 46, Loss: 0.9478, Train: 0.9623,Test: 0.9638\n",
      "Epoch: 47, Loss: 0.9478, Train: 0.9625,Test: 0.9635\n",
      "Epoch: 48, Loss: 0.9477, Train: 0.9625,Test: 0.9629\n",
      "Epoch: 49, Loss: 0.9475, Train: 0.9627,Test: 0.9626\n",
      "Epoch: 50, Loss: 0.9474, Train: 0.9629,Test: 0.9614\n",
      "Epoch: 51, Loss: 0.9474, Train: 0.9629,Test: 0.9614\n",
      "Epoch: 52, Loss: 0.9473, Train: 0.9630,Test: 0.9614\n",
      "Epoch: 53, Loss: 0.9473, Train: 0.9630,Test: 0.9614\n",
      "Epoch: 54, Loss: 0.9472, Train: 0.9632,Test: 0.9620\n",
      "Epoch: 55, Loss: 0.9471, Train: 0.9635,Test: 0.9623\n",
      "Epoch: 56, Loss: 0.9470, Train: 0.9638,Test: 0.9620\n",
      "Epoch: 57, Loss: 0.9469, Train: 0.9641,Test: 0.9620\n",
      "Epoch: 58, Loss: 0.9469, Train: 0.9645,Test: 0.9629\n",
      "Epoch: 59, Loss: 0.9468, Train: 0.9648,Test: 0.9632\n",
      "Epoch: 60, Loss: 0.9467, Train: 0.9651,Test: 0.9632\n",
      "Epoch: 61, Loss: 0.9466, Train: 0.9653,Test: 0.9635\n",
      "Epoch: 62, Loss: 0.9465, Train: 0.9654,Test: 0.9638\n",
      "Epoch: 63, Loss: 0.9463, Train: 0.9657,Test: 0.9638\n",
      "Epoch: 64, Loss: 0.9462, Train: 0.9659,Test: 0.9632\n",
      "Epoch: 65, Loss: 0.9460, Train: 0.9660,Test: 0.9635\n",
      "Epoch: 66, Loss: 0.9458, Train: 0.9661,Test: 0.9632\n",
      "Epoch: 67, Loss: 0.9457, Train: 0.9661,Test: 0.9632\n",
      "Epoch: 68, Loss: 0.9456, Train: 0.9664,Test: 0.9635\n",
      "Epoch: 69, Loss: 0.9454, Train: 0.9665,Test: 0.9638\n",
      "Epoch: 70, Loss: 0.9453, Train: 0.9664,Test: 0.9646\n",
      "Epoch: 71, Loss: 0.9452, Train: 0.9665,Test: 0.9646\n",
      "Epoch: 72, Loss: 0.9451, Train: 0.9667,Test: 0.9643\n",
      "Epoch: 73, Loss: 0.9450, Train: 0.9669,Test: 0.9646\n",
      "Epoch: 74, Loss: 0.9449, Train: 0.9669,Test: 0.9649\n",
      "Epoch: 75, Loss: 0.9447, Train: 0.9670,Test: 0.9649\n",
      "Epoch: 76, Loss: 0.9446, Train: 0.9671,Test: 0.9643\n",
      "Epoch: 77, Loss: 0.9445, Train: 0.9672,Test: 0.9641\n",
      "Epoch: 78, Loss: 0.9444, Train: 0.9674,Test: 0.9641\n",
      "Epoch: 79, Loss: 0.9443, Train: 0.9676,Test: 0.9641\n",
      "Epoch: 80, Loss: 0.9441, Train: 0.9676,Test: 0.9641\n",
      "Epoch: 81, Loss: 0.9440, Train: 0.9677,Test: 0.9643\n",
      "Epoch: 82, Loss: 0.9439, Train: 0.9679,Test: 0.9646\n",
      "Epoch: 83, Loss: 0.9438, Train: 0.9679,Test: 0.9649\n",
      "Epoch: 84, Loss: 0.9437, Train: 0.9680,Test: 0.9652\n",
      "Epoch: 85, Loss: 0.9436, Train: 0.9681,Test: 0.9649\n",
      "Epoch: 86, Loss: 0.9435, Train: 0.9682,Test: 0.9649\n",
      "Epoch: 87, Loss: 0.9433, Train: 0.9683,Test: 0.9652\n",
      "Epoch: 88, Loss: 0.9432, Train: 0.9684,Test: 0.9649\n",
      "Epoch: 89, Loss: 0.9431, Train: 0.9684,Test: 0.9649\n",
      "Epoch: 90, Loss: 0.9430, Train: 0.9685,Test: 0.9649\n",
      "Epoch: 91, Loss: 0.9429, Train: 0.9686,Test: 0.9655\n",
      "Epoch: 92, Loss: 0.9428, Train: 0.9687,Test: 0.9655\n",
      "Epoch: 93, Loss: 0.9427, Train: 0.9688,Test: 0.9658\n",
      "Epoch: 94, Loss: 0.9426, Train: 0.9690,Test: 0.9658\n",
      "Epoch: 95, Loss: 0.9425, Train: 0.9691,Test: 0.9661\n",
      "Epoch: 96, Loss: 0.9424, Train: 0.9693,Test: 0.9661\n",
      "Epoch: 97, Loss: 0.9423, Train: 0.9693,Test: 0.9661\n",
      "Epoch: 98, Loss: 0.9422, Train: 0.9693,Test: 0.9658\n",
      "Epoch: 99, Loss: 0.9421, Train: 0.9694,Test: 0.9658\n",
      "[0.1, 95, 0.9425058960914612, 0.9690751538188964, 0.9660869565217391]\n",
      "Epoch: 0, Loss: 1.6106, Train: 0.7864,Test: 0.7928\n",
      "Epoch: 1, Loss: 1.5636, Train: 0.8041,Test: 0.8099\n",
      "Epoch: 2, Loss: 1.4533, Train: 0.8011,Test: 0.8038\n",
      "Epoch: 3, Loss: 1.3327, Train: 0.8028,Test: 0.8055\n",
      "Epoch: 4, Loss: 1.2373, Train: 0.8133,Test: 0.8154\n",
      "Epoch: 5, Loss: 1.1702, Train: 0.8333,Test: 0.8351\n",
      "Epoch: 6, Loss: 1.1259, Train: 0.8567,Test: 0.8612\n",
      "Epoch: 7, Loss: 1.0932, Train: 0.8753,Test: 0.8797\n",
      "Epoch: 8, Loss: 1.0673, Train: 0.8896,Test: 0.8930\n",
      "Epoch: 9, Loss: 1.0461, Train: 0.9018,Test: 0.9070\n",
      "Epoch: 10, Loss: 1.0277, Train: 0.9130,Test: 0.9191\n",
      "Epoch: 11, Loss: 1.0118, Train: 0.9228,Test: 0.9296\n",
      "Epoch: 12, Loss: 0.9989, Train: 0.9308,Test: 0.9333\n",
      "Epoch: 13, Loss: 0.9895, Train: 0.9356,Test: 0.9388\n",
      "Epoch: 14, Loss: 0.9830, Train: 0.9397,Test: 0.9397\n",
      "Epoch: 15, Loss: 0.9776, Train: 0.9427,Test: 0.9429\n",
      "Epoch: 16, Loss: 0.9725, Train: 0.9460,Test: 0.9490\n",
      "Epoch: 17, Loss: 0.9678, Train: 0.9490,Test: 0.9493\n",
      "Epoch: 18, Loss: 0.9642, Train: 0.9507,Test: 0.9530\n",
      "Epoch: 19, Loss: 0.9621, Train: 0.9512,Test: 0.9536\n",
      "Epoch: 20, Loss: 0.9609, Train: 0.9510,Test: 0.9519\n",
      "Epoch: 21, Loss: 0.9602, Train: 0.9517,Test: 0.9516\n",
      "Epoch: 22, Loss: 0.9592, Train: 0.9525,Test: 0.9519\n",
      "Epoch: 23, Loss: 0.9578, Train: 0.9536,Test: 0.9533\n",
      "Epoch: 24, Loss: 0.9563, Train: 0.9544,Test: 0.9554\n",
      "Epoch: 25, Loss: 0.9549, Train: 0.9550,Test: 0.9580\n",
      "Epoch: 26, Loss: 0.9537, Train: 0.9557,Test: 0.9580\n",
      "Epoch: 27, Loss: 0.9528, Train: 0.9563,Test: 0.9574\n",
      "Epoch: 28, Loss: 0.9522, Train: 0.9559,Test: 0.9562\n",
      "Epoch: 29, Loss: 0.9519, Train: 0.9561,Test: 0.9568\n",
      "Epoch: 30, Loss: 0.9517, Train: 0.9563,Test: 0.9571\n",
      "Epoch: 31, Loss: 0.9514, Train: 0.9566,Test: 0.9574\n",
      "Epoch: 32, Loss: 0.9511, Train: 0.9572,Test: 0.9586\n",
      "Epoch: 33, Loss: 0.9507, Train: 0.9574,Test: 0.9586\n",
      "Epoch: 34, Loss: 0.9502, Train: 0.9579,Test: 0.9594\n",
      "Epoch: 35, Loss: 0.9497, Train: 0.9588,Test: 0.9614\n",
      "Epoch: 36, Loss: 0.9492, Train: 0.9590,Test: 0.9609\n",
      "Epoch: 37, Loss: 0.9489, Train: 0.9596,Test: 0.9620\n",
      "Epoch: 38, Loss: 0.9487, Train: 0.9596,Test: 0.9623\n",
      "Epoch: 39, Loss: 0.9486, Train: 0.9602,Test: 0.9629\n",
      "Epoch: 40, Loss: 0.9485, Train: 0.9603,Test: 0.9626\n",
      "Epoch: 41, Loss: 0.9484, Train: 0.9611,Test: 0.9626\n",
      "Epoch: 42, Loss: 0.9482, Train: 0.9613,Test: 0.9620\n",
      "Epoch: 43, Loss: 0.9480, Train: 0.9616,Test: 0.9617\n",
      "Epoch: 44, Loss: 0.9478, Train: 0.9617,Test: 0.9614\n",
      "Epoch: 45, Loss: 0.9477, Train: 0.9618,Test: 0.9612\n",
      "Epoch: 46, Loss: 0.9477, Train: 0.9618,Test: 0.9612\n",
      "Epoch: 47, Loss: 0.9476, Train: 0.9619,Test: 0.9614\n",
      "Epoch: 48, Loss: 0.9476, Train: 0.9619,Test: 0.9612\n",
      "Epoch: 49, Loss: 0.9475, Train: 0.9623,Test: 0.9617\n",
      "Epoch: 50, Loss: 0.9474, Train: 0.9626,Test: 0.9620\n",
      "Epoch: 51, Loss: 0.9472, Train: 0.9626,Test: 0.9623\n",
      "Epoch: 52, Loss: 0.9471, Train: 0.9628,Test: 0.9626\n",
      "Epoch: 53, Loss: 0.9471, Train: 0.9631,Test: 0.9638\n",
      "Epoch: 54, Loss: 0.9470, Train: 0.9634,Test: 0.9626\n",
      "Epoch: 55, Loss: 0.9469, Train: 0.9638,Test: 0.9626\n",
      "Epoch: 56, Loss: 0.9468, Train: 0.9640,Test: 0.9626\n",
      "Epoch: 57, Loss: 0.9467, Train: 0.9643,Test: 0.9626\n",
      "Epoch: 58, Loss: 0.9466, Train: 0.9644,Test: 0.9623\n",
      "Epoch: 59, Loss: 0.9465, Train: 0.9645,Test: 0.9623\n",
      "Epoch: 60, Loss: 0.9465, Train: 0.9646,Test: 0.9623\n",
      "Epoch: 61, Loss: 0.9464, Train: 0.9648,Test: 0.9629\n",
      "Epoch: 62, Loss: 0.9463, Train: 0.9650,Test: 0.9638\n",
      "Epoch: 63, Loss: 0.9461, Train: 0.9652,Test: 0.9635\n",
      "Epoch: 64, Loss: 0.9460, Train: 0.9653,Test: 0.9635\n",
      "Epoch: 65, Loss: 0.9459, Train: 0.9654,Test: 0.9632\n",
      "Epoch: 66, Loss: 0.9457, Train: 0.9658,Test: 0.9641\n",
      "Epoch: 67, Loss: 0.9455, Train: 0.9659,Test: 0.9641\n",
      "Epoch: 68, Loss: 0.9454, Train: 0.9661,Test: 0.9638\n",
      "Epoch: 69, Loss: 0.9453, Train: 0.9662,Test: 0.9638\n",
      "Epoch: 70, Loss: 0.9451, Train: 0.9662,Test: 0.9641\n",
      "Epoch: 71, Loss: 0.9450, Train: 0.9664,Test: 0.9635\n",
      "Epoch: 72, Loss: 0.9449, Train: 0.9666,Test: 0.9638\n",
      "Epoch: 73, Loss: 0.9447, Train: 0.9667,Test: 0.9641\n",
      "Epoch: 74, Loss: 0.9446, Train: 0.9668,Test: 0.9641\n",
      "Epoch: 75, Loss: 0.9445, Train: 0.9669,Test: 0.9641\n",
      "Epoch: 76, Loss: 0.9444, Train: 0.9672,Test: 0.9641\n",
      "Epoch: 77, Loss: 0.9442, Train: 0.9674,Test: 0.9643\n",
      "Epoch: 78, Loss: 0.9441, Train: 0.9673,Test: 0.9643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79, Loss: 0.9440, Train: 0.9674,Test: 0.9643\n",
      "Epoch: 80, Loss: 0.9438, Train: 0.9676,Test: 0.9649\n",
      "Epoch: 81, Loss: 0.9437, Train: 0.9677,Test: 0.9652\n",
      "Epoch: 82, Loss: 0.9435, Train: 0.9677,Test: 0.9649\n",
      "Epoch: 83, Loss: 0.9434, Train: 0.9679,Test: 0.9649\n",
      "Epoch: 84, Loss: 0.9433, Train: 0.9679,Test: 0.9652\n",
      "Epoch: 85, Loss: 0.9431, Train: 0.9681,Test: 0.9649\n",
      "Epoch: 86, Loss: 0.9430, Train: 0.9683,Test: 0.9649\n",
      "Epoch: 87, Loss: 0.9429, Train: 0.9683,Test: 0.9655\n",
      "Epoch: 88, Loss: 0.9427, Train: 0.9685,Test: 0.9655\n",
      "Epoch: 89, Loss: 0.9426, Train: 0.9688,Test: 0.9652\n",
      "Epoch: 90, Loss: 0.9425, Train: 0.9690,Test: 0.9652\n",
      "Epoch: 91, Loss: 0.9423, Train: 0.9690,Test: 0.9652\n",
      "Epoch: 92, Loss: 0.9422, Train: 0.9690,Test: 0.9655\n",
      "Epoch: 93, Loss: 0.9421, Train: 0.9691,Test: 0.9655\n",
      "Epoch: 94, Loss: 0.9419, Train: 0.9691,Test: 0.9655\n",
      "Epoch: 95, Loss: 0.9418, Train: 0.9695,Test: 0.9652\n",
      "Epoch: 96, Loss: 0.9417, Train: 0.9697,Test: 0.9649\n",
      "Epoch: 97, Loss: 0.9416, Train: 0.9698,Test: 0.9649\n",
      "Epoch: 98, Loss: 0.9415, Train: 0.9698,Test: 0.9649\n",
      "Epoch: 99, Loss: 0.9414, Train: 0.9698,Test: 0.9649\n",
      "[0.1, 87, 0.9428571462631226, 0.9683342460458074, 0.9655072463768116]\n",
      "Epoch: 0, Loss: 1.6101, Train: 0.7607,Test: 0.7614\n",
      "Epoch: 1, Loss: 1.5266, Train: 0.7473,Test: 0.7470\n",
      "Epoch: 2, Loss: 1.3833, Train: 0.7598,Test: 0.7594\n",
      "Epoch: 3, Loss: 1.2624, Train: 0.7952,Test: 0.7974\n",
      "Epoch: 4, Loss: 1.1794, Train: 0.8191,Test: 0.8214\n",
      "Epoch: 5, Loss: 1.1235, Train: 0.8297,Test: 0.8330\n",
      "Epoch: 6, Loss: 1.0880, Train: 0.8437,Test: 0.8464\n",
      "Epoch: 7, Loss: 1.0663, Train: 0.8605,Test: 0.8629\n",
      "Epoch: 8, Loss: 1.0502, Train: 0.8838,Test: 0.8899\n",
      "Epoch: 9, Loss: 1.0347, Train: 0.9046,Test: 0.9101\n",
      "Epoch: 10, Loss: 1.0193, Train: 0.9189,Test: 0.9241\n",
      "Epoch: 11, Loss: 1.0050, Train: 0.9280,Test: 0.9333\n",
      "Epoch: 12, Loss: 0.9930, Train: 0.9355,Test: 0.9388\n",
      "Epoch: 13, Loss: 0.9837, Train: 0.9424,Test: 0.9429\n",
      "Epoch: 14, Loss: 0.9771, Train: 0.9473,Test: 0.9481\n",
      "Epoch: 15, Loss: 0.9725, Train: 0.9499,Test: 0.9499\n",
      "Epoch: 16, Loss: 0.9698, Train: 0.9494,Test: 0.9487\n",
      "Epoch: 17, Loss: 0.9685, Train: 0.9481,Test: 0.9484\n",
      "Epoch: 18, Loss: 0.9672, Train: 0.9487,Test: 0.9487\n",
      "Epoch: 19, Loss: 0.9648, Train: 0.9509,Test: 0.9504\n",
      "Epoch: 20, Loss: 0.9618, Train: 0.9532,Test: 0.9530\n",
      "Epoch: 21, Loss: 0.9589, Train: 0.9543,Test: 0.9548\n",
      "Epoch: 22, Loss: 0.9567, Train: 0.9543,Test: 0.9574\n",
      "Epoch: 23, Loss: 0.9555, Train: 0.9536,Test: 0.9580\n",
      "Epoch: 24, Loss: 0.9549, Train: 0.9536,Test: 0.9586\n",
      "Epoch: 25, Loss: 0.9546, Train: 0.9540,Test: 0.9600\n",
      "Epoch: 26, Loss: 0.9543, Train: 0.9546,Test: 0.9603\n",
      "Epoch: 27, Loss: 0.9537, Train: 0.9556,Test: 0.9609\n",
      "Epoch: 28, Loss: 0.9530, Train: 0.9565,Test: 0.9614\n",
      "Epoch: 29, Loss: 0.9521, Train: 0.9572,Test: 0.9623\n",
      "Epoch: 30, Loss: 0.9512, Train: 0.9581,Test: 0.9626\n",
      "Epoch: 31, Loss: 0.9505, Train: 0.9586,Test: 0.9626\n",
      "Epoch: 32, Loss: 0.9500, Train: 0.9593,Test: 0.9617\n",
      "Epoch: 33, Loss: 0.9496, Train: 0.9594,Test: 0.9606\n",
      "Epoch: 34, Loss: 0.9495, Train: 0.9597,Test: 0.9600\n",
      "Epoch: 35, Loss: 0.9494, Train: 0.9598,Test: 0.9586\n",
      "Epoch: 36, Loss: 0.9493, Train: 0.9602,Test: 0.9588\n",
      "Epoch: 37, Loss: 0.9492, Train: 0.9605,Test: 0.9600\n",
      "Epoch: 38, Loss: 0.9490, Train: 0.9607,Test: 0.9600\n",
      "Epoch: 39, Loss: 0.9487, Train: 0.9613,Test: 0.9600\n",
      "Epoch: 40, Loss: 0.9485, Train: 0.9615,Test: 0.9609\n",
      "Epoch: 41, Loss: 0.9483, Train: 0.9616,Test: 0.9614\n",
      "Epoch: 42, Loss: 0.9482, Train: 0.9618,Test: 0.9617\n",
      "Epoch: 43, Loss: 0.9481, Train: 0.9618,Test: 0.9620\n",
      "Epoch: 44, Loss: 0.9480, Train: 0.9614,Test: 0.9623\n",
      "Epoch: 45, Loss: 0.9480, Train: 0.9616,Test: 0.9620\n",
      "Epoch: 46, Loss: 0.9480, Train: 0.9619,Test: 0.9617\n",
      "Epoch: 47, Loss: 0.9479, Train: 0.9622,Test: 0.9623\n",
      "Epoch: 48, Loss: 0.9478, Train: 0.9623,Test: 0.9626\n",
      "Epoch: 49, Loss: 0.9477, Train: 0.9627,Test: 0.9623\n",
      "Epoch: 50, Loss: 0.9476, Train: 0.9629,Test: 0.9620\n",
      "Epoch: 51, Loss: 0.9475, Train: 0.9632,Test: 0.9617\n",
      "Epoch: 52, Loss: 0.9474, Train: 0.9634,Test: 0.9606\n",
      "Epoch: 53, Loss: 0.9474, Train: 0.9633,Test: 0.9606\n",
      "Epoch: 54, Loss: 0.9473, Train: 0.9633,Test: 0.9606\n",
      "Epoch: 55, Loss: 0.9473, Train: 0.9635,Test: 0.9606\n",
      "Epoch: 56, Loss: 0.9472, Train: 0.9636,Test: 0.9606\n",
      "Epoch: 57, Loss: 0.9471, Train: 0.9641,Test: 0.9609\n",
      "Epoch: 58, Loss: 0.9470, Train: 0.9645,Test: 0.9614\n",
      "Epoch: 59, Loss: 0.9469, Train: 0.9645,Test: 0.9620\n",
      "Epoch: 60, Loss: 0.9468, Train: 0.9647,Test: 0.9620\n",
      "Epoch: 61, Loss: 0.9467, Train: 0.9648,Test: 0.9626\n",
      "Epoch: 62, Loss: 0.9466, Train: 0.9650,Test: 0.9629\n",
      "Epoch: 63, Loss: 0.9465, Train: 0.9653,Test: 0.9629\n",
      "Epoch: 64, Loss: 0.9464, Train: 0.9656,Test: 0.9629\n",
      "Epoch: 65, Loss: 0.9462, Train: 0.9656,Test: 0.9629\n",
      "Epoch: 66, Loss: 0.9461, Train: 0.9657,Test: 0.9623\n",
      "Epoch: 67, Loss: 0.9460, Train: 0.9658,Test: 0.9623\n",
      "Epoch: 68, Loss: 0.9459, Train: 0.9659,Test: 0.9623\n",
      "Epoch: 69, Loss: 0.9458, Train: 0.9661,Test: 0.9629\n",
      "Epoch: 70, Loss: 0.9457, Train: 0.9663,Test: 0.9635\n",
      "Epoch: 71, Loss: 0.9455, Train: 0.9664,Test: 0.9635\n",
      "Epoch: 72, Loss: 0.9454, Train: 0.9667,Test: 0.9632\n",
      "Epoch: 73, Loss: 0.9453, Train: 0.9668,Test: 0.9635\n",
      "Epoch: 74, Loss: 0.9452, Train: 0.9668,Test: 0.9635\n",
      "Epoch: 75, Loss: 0.9450, Train: 0.9670,Test: 0.9638\n",
      "Epoch: 76, Loss: 0.9448, Train: 0.9671,Test: 0.9638\n",
      "Epoch: 77, Loss: 0.9446, Train: 0.9672,Test: 0.9641\n",
      "Epoch: 78, Loss: 0.9445, Train: 0.9674,Test: 0.9641\n",
      "Epoch: 79, Loss: 0.9444, Train: 0.9676,Test: 0.9638\n",
      "Epoch: 80, Loss: 0.9443, Train: 0.9676,Test: 0.9638\n",
      "Epoch: 81, Loss: 0.9442, Train: 0.9678,Test: 0.9638\n",
      "Epoch: 82, Loss: 0.9440, Train: 0.9680,Test: 0.9643\n",
      "Epoch: 83, Loss: 0.9439, Train: 0.9679,Test: 0.9643\n",
      "Epoch: 84, Loss: 0.9438, Train: 0.9679,Test: 0.9646\n",
      "Epoch: 85, Loss: 0.9437, Train: 0.9682,Test: 0.9646\n",
      "Epoch: 86, Loss: 0.9436, Train: 0.9683,Test: 0.9646\n",
      "Epoch: 87, Loss: 0.9435, Train: 0.9683,Test: 0.9646\n",
      "Epoch: 88, Loss: 0.9434, Train: 0.9683,Test: 0.9646\n",
      "Epoch: 89, Loss: 0.9433, Train: 0.9685,Test: 0.9643\n",
      "Epoch: 90, Loss: 0.9432, Train: 0.9686,Test: 0.9643\n",
      "Epoch: 91, Loss: 0.9431, Train: 0.9686,Test: 0.9646\n",
      "Epoch: 92, Loss: 0.9430, Train: 0.9687,Test: 0.9646\n",
      "Epoch: 93, Loss: 0.9429, Train: 0.9687,Test: 0.9649\n",
      "Epoch: 94, Loss: 0.9428, Train: 0.9688,Test: 0.9652\n",
      "Epoch: 95, Loss: 0.9427, Train: 0.9690,Test: 0.9655\n",
      "Epoch: 96, Loss: 0.9426, Train: 0.9690,Test: 0.9658\n",
      "Epoch: 97, Loss: 0.9425, Train: 0.9691,Test: 0.9658\n",
      "Epoch: 98, Loss: 0.9424, Train: 0.9692,Test: 0.9658\n",
      "Epoch: 99, Loss: 0.9423, Train: 0.9692,Test: 0.9664\n",
      "[0.1, 99, 0.9422887563705444, 0.9692362207260896, 0.9663768115942029]\n",
      "Epoch: 0, Loss: 1.6062, Train: 0.6617,Test: 0.6620\n",
      "Epoch: 1, Loss: 1.4472, Train: 0.6687,Test: 0.6701\n",
      "Epoch: 2, Loss: 1.2895, Train: 0.6867,Test: 0.6884\n",
      "Epoch: 3, Loss: 1.2123, Train: 0.7215,Test: 0.7270\n",
      "Epoch: 4, Loss: 1.1689, Train: 0.7844,Test: 0.7910\n",
      "Epoch: 5, Loss: 1.1323, Train: 0.8165,Test: 0.8238\n",
      "Epoch: 6, Loss: 1.1028, Train: 0.8343,Test: 0.8380\n",
      "Epoch: 7, Loss: 1.0835, Train: 0.8464,Test: 0.8501\n",
      "Epoch: 8, Loss: 1.0703, Train: 0.8636,Test: 0.8707\n",
      "Epoch: 9, Loss: 1.0571, Train: 0.8866,Test: 0.8910\n",
      "Epoch: 10, Loss: 1.0415, Train: 0.9105,Test: 0.9186\n",
      "Epoch: 11, Loss: 1.0246, Train: 0.9225,Test: 0.9275\n",
      "Epoch: 12, Loss: 1.0091, Train: 0.9306,Test: 0.9316\n",
      "Epoch: 13, Loss: 0.9966, Train: 0.9364,Test: 0.9377\n",
      "Epoch: 14, Loss: 0.9876, Train: 0.9428,Test: 0.9446\n",
      "Epoch: 15, Loss: 0.9813, Train: 0.9473,Test: 0.9493\n",
      "Epoch: 16, Loss: 0.9772, Train: 0.9482,Test: 0.9458\n",
      "Epoch: 17, Loss: 0.9743, Train: 0.9488,Test: 0.9472\n",
      "Epoch: 18, Loss: 0.9715, Train: 0.9492,Test: 0.9490\n",
      "Epoch: 19, Loss: 0.9681, Train: 0.9509,Test: 0.9513\n",
      "Epoch: 20, Loss: 0.9643, Train: 0.9522,Test: 0.9542\n",
      "Epoch: 21, Loss: 0.9609, Train: 0.9528,Test: 0.9536\n",
      "Epoch: 22, Loss: 0.9585, Train: 0.9534,Test: 0.9559\n",
      "Epoch: 23, Loss: 0.9571, Train: 0.9535,Test: 0.9574\n",
      "Epoch: 24, Loss: 0.9563, Train: 0.9532,Test: 0.9562\n",
      "Epoch: 25, Loss: 0.9559, Train: 0.9535,Test: 0.9562\n",
      "Epoch: 26, Loss: 0.9553, Train: 0.9539,Test: 0.9568\n",
      "Epoch: 27, Loss: 0.9546, Train: 0.9545,Test: 0.9571\n",
      "Epoch: 28, Loss: 0.9538, Train: 0.9549,Test: 0.9577\n",
      "Epoch: 29, Loss: 0.9529, Train: 0.9555,Test: 0.9580\n",
      "Epoch: 30, Loss: 0.9521, Train: 0.9564,Test: 0.9577\n",
      "Epoch: 31, Loss: 0.9514, Train: 0.9574,Test: 0.9586\n",
      "Epoch: 32, Loss: 0.9508, Train: 0.9577,Test: 0.9586\n",
      "Epoch: 33, Loss: 0.9504, Train: 0.9579,Test: 0.9591\n",
      "Epoch: 34, Loss: 0.9501, Train: 0.9587,Test: 0.9591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Loss: 0.9499, Train: 0.9591,Test: 0.9597\n",
      "Epoch: 36, Loss: 0.9497, Train: 0.9594,Test: 0.9600\n",
      "Epoch: 37, Loss: 0.9494, Train: 0.9598,Test: 0.9609\n",
      "Epoch: 38, Loss: 0.9491, Train: 0.9606,Test: 0.9617\n",
      "Epoch: 39, Loss: 0.9488, Train: 0.9605,Test: 0.9620\n",
      "Epoch: 40, Loss: 0.9485, Train: 0.9607,Test: 0.9635\n",
      "Epoch: 41, Loss: 0.9484, Train: 0.9607,Test: 0.9626\n",
      "Epoch: 42, Loss: 0.9483, Train: 0.9607,Test: 0.9629\n",
      "Epoch: 43, Loss: 0.9482, Train: 0.9607,Test: 0.9623\n",
      "Epoch: 44, Loss: 0.9482, Train: 0.9610,Test: 0.9623\n",
      "Epoch: 45, Loss: 0.9480, Train: 0.9615,Test: 0.9620\n",
      "Epoch: 46, Loss: 0.9479, Train: 0.9617,Test: 0.9620\n",
      "Epoch: 47, Loss: 0.9477, Train: 0.9619,Test: 0.9623\n",
      "Epoch: 48, Loss: 0.9476, Train: 0.9620,Test: 0.9617\n",
      "Epoch: 49, Loss: 0.9476, Train: 0.9623,Test: 0.9623\n",
      "Epoch: 50, Loss: 0.9476, Train: 0.9625,Test: 0.9620\n",
      "Epoch: 51, Loss: 0.9475, Train: 0.9630,Test: 0.9629\n",
      "Epoch: 52, Loss: 0.9474, Train: 0.9631,Test: 0.9632\n",
      "Epoch: 53, Loss: 0.9474, Train: 0.9634,Test: 0.9629\n",
      "Epoch: 54, Loss: 0.9473, Train: 0.9635,Test: 0.9635\n",
      "Epoch: 55, Loss: 0.9472, Train: 0.9636,Test: 0.9638\n",
      "Epoch: 56, Loss: 0.9472, Train: 0.9638,Test: 0.9638\n",
      "Epoch: 57, Loss: 0.9471, Train: 0.9643,Test: 0.9641\n",
      "Epoch: 58, Loss: 0.9470, Train: 0.9645,Test: 0.9638\n",
      "Epoch: 59, Loss: 0.9469, Train: 0.9648,Test: 0.9635\n",
      "Epoch: 60, Loss: 0.9468, Train: 0.9650,Test: 0.9635\n",
      "Epoch: 61, Loss: 0.9467, Train: 0.9650,Test: 0.9629\n",
      "Epoch: 62, Loss: 0.9466, Train: 0.9652,Test: 0.9626\n",
      "Epoch: 63, Loss: 0.9465, Train: 0.9656,Test: 0.9635\n",
      "Epoch: 64, Loss: 0.9464, Train: 0.9655,Test: 0.9641\n",
      "Epoch: 65, Loss: 0.9462, Train: 0.9658,Test: 0.9643\n",
      "Epoch: 66, Loss: 0.9460, Train: 0.9660,Test: 0.9643\n",
      "Epoch: 67, Loss: 0.9459, Train: 0.9663,Test: 0.9643\n",
      "Epoch: 68, Loss: 0.9458, Train: 0.9662,Test: 0.9646\n",
      "Epoch: 69, Loss: 0.9457, Train: 0.9663,Test: 0.9643\n",
      "Epoch: 70, Loss: 0.9455, Train: 0.9665,Test: 0.9641\n",
      "Epoch: 71, Loss: 0.9454, Train: 0.9666,Test: 0.9638\n",
      "Epoch: 72, Loss: 0.9453, Train: 0.9668,Test: 0.9638\n",
      "Epoch: 73, Loss: 0.9452, Train: 0.9669,Test: 0.9638\n",
      "Epoch: 74, Loss: 0.9451, Train: 0.9670,Test: 0.9646\n",
      "Epoch: 75, Loss: 0.9450, Train: 0.9670,Test: 0.9643\n",
      "Epoch: 76, Loss: 0.9449, Train: 0.9670,Test: 0.9646\n",
      "Epoch: 77, Loss: 0.9448, Train: 0.9672,Test: 0.9646\n",
      "Epoch: 78, Loss: 0.9447, Train: 0.9674,Test: 0.9643\n",
      "Epoch: 79, Loss: 0.9445, Train: 0.9675,Test: 0.9643\n",
      "Epoch: 80, Loss: 0.9444, Train: 0.9676,Test: 0.9643\n",
      "Epoch: 81, Loss: 0.9443, Train: 0.9677,Test: 0.9641\n",
      "Epoch: 82, Loss: 0.9442, Train: 0.9677,Test: 0.9641\n",
      "Epoch: 83, Loss: 0.9441, Train: 0.9677,Test: 0.9643\n",
      "Epoch: 84, Loss: 0.9440, Train: 0.9678,Test: 0.9646\n",
      "Epoch: 85, Loss: 0.9439, Train: 0.9679,Test: 0.9643\n",
      "Epoch: 86, Loss: 0.9438, Train: 0.9680,Test: 0.9641\n",
      "Epoch: 87, Loss: 0.9437, Train: 0.9680,Test: 0.9641\n",
      "Epoch: 88, Loss: 0.9436, Train: 0.9682,Test: 0.9641\n",
      "Epoch: 89, Loss: 0.9435, Train: 0.9683,Test: 0.9643\n",
      "Epoch: 90, Loss: 0.9434, Train: 0.9683,Test: 0.9643\n",
      "Epoch: 91, Loss: 0.9433, Train: 0.9684,Test: 0.9646\n",
      "Epoch: 92, Loss: 0.9432, Train: 0.9685,Test: 0.9649\n",
      "Epoch: 93, Loss: 0.9431, Train: 0.9687,Test: 0.9649\n",
      "Epoch: 94, Loss: 0.9430, Train: 0.9688,Test: 0.9652\n",
      "Epoch: 95, Loss: 0.9429, Train: 0.9688,Test: 0.9652\n",
      "Epoch: 96, Loss: 0.9428, Train: 0.9689,Test: 0.9652\n",
      "Epoch: 97, Loss: 0.9427, Train: 0.9690,Test: 0.9649\n",
      "Epoch: 98, Loss: 0.9426, Train: 0.9690,Test: 0.9649\n",
      "Epoch: 99, Loss: 0.9426, Train: 0.9692,Test: 0.9649\n",
      "[0.1, 94, 0.9430090188980103, 0.9688174467673871, 0.9652173913043478]\n",
      "Epoch: 0, Loss: 1.6100, Train: 0.7486,Test: 0.7528\n",
      "Epoch: 1, Loss: 1.5468, Train: 0.7314,Test: 0.7351\n",
      "Epoch: 2, Loss: 1.4107, Train: 0.7249,Test: 0.7287\n",
      "Epoch: 3, Loss: 1.2859, Train: 0.7377,Test: 0.7400\n",
      "Epoch: 4, Loss: 1.2030, Train: 0.7690,Test: 0.7742\n",
      "Epoch: 5, Loss: 1.1555, Train: 0.8133,Test: 0.8159\n",
      "Epoch: 6, Loss: 1.1150, Train: 0.8653,Test: 0.8687\n",
      "Epoch: 7, Loss: 1.0746, Train: 0.9104,Test: 0.9145\n",
      "Epoch: 8, Loss: 1.0386, Train: 0.9324,Test: 0.9336\n",
      "Epoch: 9, Loss: 1.0128, Train: 0.9398,Test: 0.9429\n",
      "Epoch: 10, Loss: 0.9972, Train: 0.9439,Test: 0.9458\n",
      "Epoch: 11, Loss: 0.9887, Train: 0.9444,Test: 0.9475\n",
      "Epoch: 12, Loss: 0.9834, Train: 0.9448,Test: 0.9458\n",
      "Epoch: 13, Loss: 0.9789, Train: 0.9460,Test: 0.9470\n",
      "Epoch: 14, Loss: 0.9739, Train: 0.9485,Test: 0.9496\n",
      "Epoch: 15, Loss: 0.9688, Train: 0.9504,Test: 0.9513\n",
      "Epoch: 16, Loss: 0.9643, Train: 0.9519,Test: 0.9528\n",
      "Epoch: 17, Loss: 0.9609, Train: 0.9527,Test: 0.9542\n",
      "Epoch: 18, Loss: 0.9587, Train: 0.9535,Test: 0.9559\n",
      "Epoch: 19, Loss: 0.9574, Train: 0.9532,Test: 0.9551\n",
      "Epoch: 20, Loss: 0.9567, Train: 0.9533,Test: 0.9539\n",
      "Epoch: 21, Loss: 0.9561, Train: 0.9535,Test: 0.9545\n",
      "Epoch: 22, Loss: 0.9554, Train: 0.9537,Test: 0.9559\n",
      "Epoch: 23, Loss: 0.9546, Train: 0.9543,Test: 0.9565\n",
      "Epoch: 24, Loss: 0.9538, Train: 0.9545,Test: 0.9568\n",
      "Epoch: 25, Loss: 0.9529, Train: 0.9554,Test: 0.9568\n",
      "Epoch: 26, Loss: 0.9521, Train: 0.9563,Test: 0.9577\n",
      "Epoch: 27, Loss: 0.9514, Train: 0.9573,Test: 0.9586\n",
      "Epoch: 28, Loss: 0.9509, Train: 0.9573,Test: 0.9606\n",
      "Epoch: 29, Loss: 0.9504, Train: 0.9576,Test: 0.9614\n",
      "Epoch: 30, Loss: 0.9502, Train: 0.9583,Test: 0.9609\n",
      "Epoch: 31, Loss: 0.9499, Train: 0.9583,Test: 0.9612\n",
      "Epoch: 32, Loss: 0.9497, Train: 0.9586,Test: 0.9609\n",
      "Epoch: 33, Loss: 0.9495, Train: 0.9594,Test: 0.9623\n",
      "Epoch: 34, Loss: 0.9492, Train: 0.9601,Test: 0.9620\n",
      "Epoch: 35, Loss: 0.9488, Train: 0.9601,Test: 0.9617\n",
      "Epoch: 36, Loss: 0.9486, Train: 0.9602,Test: 0.9620\n",
      "Epoch: 37, Loss: 0.9484, Train: 0.9602,Test: 0.9612\n",
      "Epoch: 38, Loss: 0.9482, Train: 0.9603,Test: 0.9612\n",
      "Epoch: 39, Loss: 0.9482, Train: 0.9605,Test: 0.9609\n",
      "Epoch: 40, Loss: 0.9481, Train: 0.9606,Test: 0.9609\n",
      "Epoch: 41, Loss: 0.9480, Train: 0.9608,Test: 0.9617\n",
      "Epoch: 42, Loss: 0.9479, Train: 0.9612,Test: 0.9620\n",
      "Epoch: 43, Loss: 0.9477, Train: 0.9616,Test: 0.9620\n",
      "Epoch: 44, Loss: 0.9476, Train: 0.9619,Test: 0.9617\n",
      "Epoch: 45, Loss: 0.9475, Train: 0.9621,Test: 0.9620\n",
      "Epoch: 46, Loss: 0.9475, Train: 0.9622,Test: 0.9626\n",
      "Epoch: 47, Loss: 0.9474, Train: 0.9624,Test: 0.9629\n",
      "Epoch: 48, Loss: 0.9474, Train: 0.9625,Test: 0.9623\n",
      "Epoch: 49, Loss: 0.9473, Train: 0.9629,Test: 0.9623\n",
      "Epoch: 50, Loss: 0.9472, Train: 0.9633,Test: 0.9623\n",
      "Epoch: 51, Loss: 0.9471, Train: 0.9636,Test: 0.9626\n",
      "Epoch: 52, Loss: 0.9471, Train: 0.9634,Test: 0.9626\n",
      "Epoch: 53, Loss: 0.9470, Train: 0.9636,Test: 0.9623\n",
      "Epoch: 54, Loss: 0.9470, Train: 0.9637,Test: 0.9623\n",
      "Epoch: 55, Loss: 0.9469, Train: 0.9640,Test: 0.9626\n",
      "Epoch: 56, Loss: 0.9468, Train: 0.9641,Test: 0.9629\n",
      "Epoch: 57, Loss: 0.9467, Train: 0.9643,Test: 0.9635\n",
      "Epoch: 58, Loss: 0.9466, Train: 0.9644,Test: 0.9638\n",
      "Epoch: 59, Loss: 0.9465, Train: 0.9644,Test: 0.9635\n",
      "Epoch: 60, Loss: 0.9464, Train: 0.9647,Test: 0.9632\n",
      "Epoch: 61, Loss: 0.9463, Train: 0.9648,Test: 0.9632\n",
      "Epoch: 62, Loss: 0.9462, Train: 0.9649,Test: 0.9629\n",
      "Epoch: 63, Loss: 0.9460, Train: 0.9654,Test: 0.9632\n",
      "Epoch: 64, Loss: 0.9458, Train: 0.9654,Test: 0.9632\n",
      "Epoch: 65, Loss: 0.9457, Train: 0.9655,Test: 0.9635\n",
      "Epoch: 66, Loss: 0.9456, Train: 0.9656,Test: 0.9632\n",
      "Epoch: 67, Loss: 0.9455, Train: 0.9658,Test: 0.9635\n",
      "Epoch: 68, Loss: 0.9454, Train: 0.9659,Test: 0.9638\n",
      "Epoch: 69, Loss: 0.9452, Train: 0.9662,Test: 0.9638\n",
      "Epoch: 70, Loss: 0.9451, Train: 0.9664,Test: 0.9641\n",
      "Epoch: 71, Loss: 0.9450, Train: 0.9666,Test: 0.9641\n",
      "Epoch: 72, Loss: 0.9449, Train: 0.9665,Test: 0.9641\n",
      "Epoch: 73, Loss: 0.9448, Train: 0.9665,Test: 0.9641\n",
      "Epoch: 74, Loss: 0.9447, Train: 0.9665,Test: 0.9641\n",
      "Epoch: 75, Loss: 0.9445, Train: 0.9666,Test: 0.9641\n",
      "Epoch: 76, Loss: 0.9444, Train: 0.9667,Test: 0.9643\n",
      "Epoch: 77, Loss: 0.9443, Train: 0.9669,Test: 0.9641\n",
      "Epoch: 78, Loss: 0.9442, Train: 0.9669,Test: 0.9643\n",
      "Epoch: 79, Loss: 0.9441, Train: 0.9670,Test: 0.9643\n",
      "Epoch: 80, Loss: 0.9439, Train: 0.9672,Test: 0.9643\n",
      "Epoch: 81, Loss: 0.9438, Train: 0.9672,Test: 0.9646\n",
      "Epoch: 82, Loss: 0.9437, Train: 0.9674,Test: 0.9646\n",
      "Epoch: 83, Loss: 0.9436, Train: 0.9675,Test: 0.9646\n",
      "Epoch: 84, Loss: 0.9435, Train: 0.9676,Test: 0.9646\n",
      "Epoch: 85, Loss: 0.9434, Train: 0.9676,Test: 0.9646\n",
      "Epoch: 86, Loss: 0.9432, Train: 0.9678,Test: 0.9649\n",
      "Epoch: 87, Loss: 0.9431, Train: 0.9680,Test: 0.9646\n",
      "Epoch: 88, Loss: 0.9430, Train: 0.9681,Test: 0.9646\n",
      "Epoch: 89, Loss: 0.9429, Train: 0.9682,Test: 0.9646\n",
      "Epoch: 90, Loss: 0.9428, Train: 0.9684,Test: 0.9646\n",
      "Epoch: 91, Loss: 0.9427, Train: 0.9684,Test: 0.9646\n",
      "Epoch: 92, Loss: 0.9426, Train: 0.9683,Test: 0.9643\n",
      "Epoch: 93, Loss: 0.9425, Train: 0.9686,Test: 0.9643\n",
      "Epoch: 94, Loss: 0.9424, Train: 0.9688,Test: 0.9649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95, Loss: 0.9423, Train: 0.9689,Test: 0.9649\n",
      "Epoch: 96, Loss: 0.9422, Train: 0.9690,Test: 0.9646\n",
      "Epoch: 97, Loss: 0.9421, Train: 0.9690,Test: 0.9646\n",
      "Epoch: 98, Loss: 0.9420, Train: 0.9692,Test: 0.9655\n",
      "Epoch: 99, Loss: 0.9419, Train: 0.9693,Test: 0.9655\n",
      "[0.1, 98, 0.9419993162155151, 0.9691717939632123, 0.9655072463768116]\n",
      "Epoch: 0, Loss: 1.6099, Train: 0.7650,Test: 0.7681\n",
      "Epoch: 1, Loss: 1.5430, Train: 0.7807,Test: 0.7786\n",
      "Epoch: 2, Loss: 1.4162, Train: 0.7875,Test: 0.7893\n",
      "Epoch: 3, Loss: 1.2914, Train: 0.8018,Test: 0.8026\n",
      "Epoch: 4, Loss: 1.1995, Train: 0.8365,Test: 0.8388\n",
      "Epoch: 5, Loss: 1.1377, Train: 0.8781,Test: 0.8800\n",
      "Epoch: 6, Loss: 1.0901, Train: 0.9047,Test: 0.9113\n",
      "Epoch: 7, Loss: 1.0519, Train: 0.9215,Test: 0.9258\n",
      "Epoch: 8, Loss: 1.0237, Train: 0.9332,Test: 0.9330\n",
      "Epoch: 9, Loss: 1.0043, Train: 0.9388,Test: 0.9429\n",
      "Epoch: 10, Loss: 0.9915, Train: 0.9441,Test: 0.9467\n",
      "Epoch: 11, Loss: 0.9832, Train: 0.9474,Test: 0.9493\n",
      "Epoch: 12, Loss: 0.9774, Train: 0.9483,Test: 0.9499\n",
      "Epoch: 13, Loss: 0.9730, Train: 0.9497,Test: 0.9507\n",
      "Epoch: 14, Loss: 0.9690, Train: 0.9508,Test: 0.9507\n",
      "Epoch: 15, Loss: 0.9653, Train: 0.9519,Test: 0.9522\n",
      "Epoch: 16, Loss: 0.9620, Train: 0.9523,Test: 0.9536\n",
      "Epoch: 17, Loss: 0.9596, Train: 0.9525,Test: 0.9562\n",
      "Epoch: 18, Loss: 0.9580, Train: 0.9521,Test: 0.9559\n",
      "Epoch: 19, Loss: 0.9569, Train: 0.9529,Test: 0.9562\n",
      "Epoch: 20, Loss: 0.9560, Train: 0.9531,Test: 0.9568\n",
      "Epoch: 21, Loss: 0.9552, Train: 0.9539,Test: 0.9559\n",
      "Epoch: 22, Loss: 0.9543, Train: 0.9547,Test: 0.9568\n",
      "Epoch: 23, Loss: 0.9535, Train: 0.9554,Test: 0.9583\n",
      "Epoch: 24, Loss: 0.9527, Train: 0.9560,Test: 0.9588\n",
      "Epoch: 25, Loss: 0.9520, Train: 0.9566,Test: 0.9588\n",
      "Epoch: 26, Loss: 0.9515, Train: 0.9571,Test: 0.9586\n",
      "Epoch: 27, Loss: 0.9510, Train: 0.9575,Test: 0.9591\n",
      "Epoch: 28, Loss: 0.9506, Train: 0.9581,Test: 0.9594\n",
      "Epoch: 29, Loss: 0.9503, Train: 0.9586,Test: 0.9597\n",
      "Epoch: 30, Loss: 0.9500, Train: 0.9588,Test: 0.9603\n",
      "Epoch: 31, Loss: 0.9498, Train: 0.9593,Test: 0.9609\n",
      "Epoch: 32, Loss: 0.9495, Train: 0.9598,Test: 0.9614\n",
      "Epoch: 33, Loss: 0.9493, Train: 0.9601,Test: 0.9620\n",
      "Epoch: 34, Loss: 0.9491, Train: 0.9602,Test: 0.9626\n",
      "Epoch: 35, Loss: 0.9488, Train: 0.9603,Test: 0.9623\n",
      "Epoch: 36, Loss: 0.9487, Train: 0.9605,Test: 0.9620\n",
      "Epoch: 37, Loss: 0.9486, Train: 0.9606,Test: 0.9617\n",
      "Epoch: 38, Loss: 0.9485, Train: 0.9606,Test: 0.9620\n",
      "Epoch: 39, Loss: 0.9484, Train: 0.9608,Test: 0.9617\n",
      "Epoch: 40, Loss: 0.9483, Train: 0.9611,Test: 0.9614\n",
      "Epoch: 41, Loss: 0.9482, Train: 0.9615,Test: 0.9612\n",
      "Epoch: 42, Loss: 0.9481, Train: 0.9616,Test: 0.9606\n",
      "Epoch: 43, Loss: 0.9480, Train: 0.9620,Test: 0.9614\n",
      "Epoch: 44, Loss: 0.9479, Train: 0.9622,Test: 0.9614\n",
      "Epoch: 45, Loss: 0.9479, Train: 0.9626,Test: 0.9620\n",
      "Epoch: 46, Loss: 0.9478, Train: 0.9627,Test: 0.9620\n",
      "Epoch: 47, Loss: 0.9478, Train: 0.9627,Test: 0.9620\n",
      "Epoch: 48, Loss: 0.9477, Train: 0.9629,Test: 0.9623\n",
      "Epoch: 49, Loss: 0.9477, Train: 0.9632,Test: 0.9629\n",
      "Epoch: 50, Loss: 0.9476, Train: 0.9632,Test: 0.9626\n",
      "Epoch: 51, Loss: 0.9475, Train: 0.9632,Test: 0.9626\n",
      "Epoch: 52, Loss: 0.9475, Train: 0.9633,Test: 0.9626\n",
      "Epoch: 53, Loss: 0.9474, Train: 0.9631,Test: 0.9626\n",
      "Epoch: 54, Loss: 0.9473, Train: 0.9635,Test: 0.9626\n",
      "Epoch: 55, Loss: 0.9472, Train: 0.9637,Test: 0.9635\n",
      "Epoch: 56, Loss: 0.9472, Train: 0.9639,Test: 0.9632\n",
      "Epoch: 57, Loss: 0.9471, Train: 0.9641,Test: 0.9632\n",
      "Epoch: 58, Loss: 0.9470, Train: 0.9643,Test: 0.9629\n",
      "Epoch: 59, Loss: 0.9469, Train: 0.9644,Test: 0.9629\n",
      "Epoch: 60, Loss: 0.9468, Train: 0.9645,Test: 0.9629\n",
      "Epoch: 61, Loss: 0.9467, Train: 0.9646,Test: 0.9629\n",
      "Epoch: 62, Loss: 0.9466, Train: 0.9647,Test: 0.9629\n",
      "Epoch: 63, Loss: 0.9465, Train: 0.9649,Test: 0.9632\n",
      "Epoch: 64, Loss: 0.9464, Train: 0.9650,Test: 0.9629\n",
      "Epoch: 65, Loss: 0.9462, Train: 0.9651,Test: 0.9629\n",
      "Epoch: 66, Loss: 0.9461, Train: 0.9656,Test: 0.9635\n",
      "Epoch: 67, Loss: 0.9459, Train: 0.9658,Test: 0.9635\n",
      "Epoch: 68, Loss: 0.9457, Train: 0.9660,Test: 0.9638\n",
      "Epoch: 69, Loss: 0.9456, Train: 0.9662,Test: 0.9641\n",
      "Epoch: 70, Loss: 0.9455, Train: 0.9663,Test: 0.9641\n",
      "Epoch: 71, Loss: 0.9453, Train: 0.9664,Test: 0.9641\n",
      "Epoch: 72, Loss: 0.9452, Train: 0.9665,Test: 0.9638\n",
      "Epoch: 73, Loss: 0.9451, Train: 0.9665,Test: 0.9638\n",
      "Epoch: 74, Loss: 0.9450, Train: 0.9665,Test: 0.9638\n",
      "Epoch: 75, Loss: 0.9449, Train: 0.9667,Test: 0.9641\n",
      "Epoch: 76, Loss: 0.9447, Train: 0.9669,Test: 0.9641\n",
      "Epoch: 77, Loss: 0.9446, Train: 0.9669,Test: 0.9641\n",
      "Epoch: 78, Loss: 0.9445, Train: 0.9670,Test: 0.9643\n",
      "Epoch: 79, Loss: 0.9444, Train: 0.9670,Test: 0.9646\n",
      "Epoch: 80, Loss: 0.9443, Train: 0.9671,Test: 0.9643\n",
      "Epoch: 81, Loss: 0.9442, Train: 0.9673,Test: 0.9643\n",
      "Epoch: 82, Loss: 0.9441, Train: 0.9675,Test: 0.9641\n",
      "Epoch: 83, Loss: 0.9439, Train: 0.9676,Test: 0.9643\n",
      "Epoch: 84, Loss: 0.9438, Train: 0.9677,Test: 0.9643\n",
      "Epoch: 85, Loss: 0.9437, Train: 0.9677,Test: 0.9649\n",
      "Epoch: 86, Loss: 0.9436, Train: 0.9678,Test: 0.9649\n",
      "Epoch: 87, Loss: 0.9435, Train: 0.9680,Test: 0.9652\n",
      "Epoch: 88, Loss: 0.9434, Train: 0.9682,Test: 0.9652\n",
      "Epoch: 89, Loss: 0.9433, Train: 0.9683,Test: 0.9646\n",
      "Epoch: 90, Loss: 0.9432, Train: 0.9684,Test: 0.9652\n",
      "Epoch: 91, Loss: 0.9431, Train: 0.9685,Test: 0.9652\n",
      "Epoch: 92, Loss: 0.9430, Train: 0.9686,Test: 0.9652\n",
      "Epoch: 93, Loss: 0.9429, Train: 0.9686,Test: 0.9652\n",
      "Epoch: 94, Loss: 0.9428, Train: 0.9687,Test: 0.9652\n",
      "Epoch: 95, Loss: 0.9427, Train: 0.9688,Test: 0.9649\n",
      "Epoch: 96, Loss: 0.9426, Train: 0.9689,Test: 0.9646\n",
      "Epoch: 97, Loss: 0.9425, Train: 0.9690,Test: 0.9646\n",
      "Epoch: 98, Loss: 0.9424, Train: 0.9690,Test: 0.9646\n",
      "Epoch: 99, Loss: 0.9423, Train: 0.9690,Test: 0.9646\n",
      "[0.1, 87, 0.943501889705658, 0.9680121122314209, 0.9652173913043478]\n",
      "Epoch: 0, Loss: 1.6098, Train: 0.8356,Test: 0.8371\n",
      "Epoch: 1, Loss: 1.5440, Train: 0.8014,Test: 0.8035\n",
      "Epoch: 2, Loss: 1.3943, Train: 0.7998,Test: 0.7980\n",
      "Epoch: 3, Loss: 1.2525, Train: 0.8247,Test: 0.8284\n",
      "Epoch: 4, Loss: 1.1615, Train: 0.8434,Test: 0.8441\n",
      "Epoch: 5, Loss: 1.1084, Train: 0.8518,Test: 0.8533\n",
      "Epoch: 6, Loss: 1.0784, Train: 0.8560,Test: 0.8591\n",
      "Epoch: 7, Loss: 1.0605, Train: 0.8645,Test: 0.8687\n",
      "Epoch: 8, Loss: 1.0465, Train: 0.8795,Test: 0.8832\n",
      "Epoch: 9, Loss: 1.0333, Train: 0.8968,Test: 0.9020\n",
      "Epoch: 10, Loss: 1.0210, Train: 0.9094,Test: 0.9139\n",
      "Epoch: 11, Loss: 1.0099, Train: 0.9191,Test: 0.9241\n",
      "Epoch: 12, Loss: 0.9998, Train: 0.9285,Test: 0.9319\n",
      "Epoch: 13, Loss: 0.9902, Train: 0.9387,Test: 0.9403\n",
      "Epoch: 14, Loss: 0.9809, Train: 0.9441,Test: 0.9452\n",
      "Epoch: 15, Loss: 0.9736, Train: 0.9479,Test: 0.9496\n",
      "Epoch: 16, Loss: 0.9697, Train: 0.9479,Test: 0.9507\n",
      "Epoch: 17, Loss: 0.9686, Train: 0.9479,Test: 0.9496\n",
      "Epoch: 18, Loss: 0.9677, Train: 0.9492,Test: 0.9519\n",
      "Epoch: 19, Loss: 0.9649, Train: 0.9526,Test: 0.9548\n",
      "Epoch: 20, Loss: 0.9609, Train: 0.9544,Test: 0.9577\n",
      "Epoch: 21, Loss: 0.9577, Train: 0.9541,Test: 0.9580\n",
      "Epoch: 22, Loss: 0.9560, Train: 0.9539,Test: 0.9557\n",
      "Epoch: 23, Loss: 0.9554, Train: 0.9537,Test: 0.9542\n",
      "Epoch: 24, Loss: 0.9551, Train: 0.9537,Test: 0.9533\n",
      "Epoch: 25, Loss: 0.9548, Train: 0.9538,Test: 0.9542\n",
      "Epoch: 26, Loss: 0.9543, Train: 0.9542,Test: 0.9548\n",
      "Epoch: 27, Loss: 0.9536, Train: 0.9553,Test: 0.9568\n",
      "Epoch: 28, Loss: 0.9528, Train: 0.9560,Test: 0.9588\n",
      "Epoch: 29, Loss: 0.9519, Train: 0.9566,Test: 0.9614\n",
      "Epoch: 30, Loss: 0.9512, Train: 0.9578,Test: 0.9623\n",
      "Epoch: 31, Loss: 0.9506, Train: 0.9582,Test: 0.9635\n",
      "Epoch: 32, Loss: 0.9502, Train: 0.9585,Test: 0.9629\n",
      "Epoch: 33, Loss: 0.9499, Train: 0.9593,Test: 0.9638\n",
      "Epoch: 34, Loss: 0.9497, Train: 0.9594,Test: 0.9638\n",
      "Epoch: 35, Loss: 0.9495, Train: 0.9598,Test: 0.9626\n",
      "Epoch: 36, Loss: 0.9493, Train: 0.9603,Test: 0.9617\n",
      "Epoch: 37, Loss: 0.9490, Train: 0.9605,Test: 0.9609\n",
      "Epoch: 38, Loss: 0.9488, Train: 0.9608,Test: 0.9612\n",
      "Epoch: 39, Loss: 0.9485, Train: 0.9612,Test: 0.9617\n",
      "Epoch: 40, Loss: 0.9483, Train: 0.9611,Test: 0.9612\n",
      "Epoch: 41, Loss: 0.9482, Train: 0.9611,Test: 0.9617\n",
      "Epoch: 42, Loss: 0.9480, Train: 0.9613,Test: 0.9626\n",
      "Epoch: 43, Loss: 0.9479, Train: 0.9614,Test: 0.9635\n",
      "Epoch: 44, Loss: 0.9479, Train: 0.9621,Test: 0.9635\n"
     ]
    }
   ],
   "source": [
    "def mynorm(tensor):\n",
    "    # \n",
    "    min_values = tensor.min(dim=1, keepdim=True)[0]\n",
    "    max_values = tensor.max(dim=1, keepdim=True)[0]\n",
    "    # -\n",
    "#     normalized_tensor = (tensor - min_values) / (max_values - min_values + 1e-8)               ### 0-1\n",
    "    normalized_tensor = 2 * (tensor - min_values) / (max_values - min_values + 1e-8) - 1       ### -1-1\n",
    "    return normalized_tensor\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_channels,64)\n",
    "        self.conv1 = GCNConv(64,32)\n",
    "        self.conv2 = GCNConv(32,out_channels)\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.children():\n",
    "            layer.reset_parameters()   \n",
    "            \n",
    "    def forward(self, x, edge_index):\n",
    "#         print(\"conv00:{}\".format(x))\n",
    "        x0 = self.fc1(x)\n",
    "        x1 = self.conv1(x0, edge_index).relu()         ##\n",
    "        x2 = self.conv2(x1, edge_index).softmax(dim=-1)\n",
    " \n",
    "        return x2      \n",
    "\n",
    "def train(model,train_set_ind,train_label_ind):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    label=torch.nn.functional.one_hot(data.y[train_label_ind], num_classes=nm.num_classes)\n",
    "    label=label.to(torch.float32)\n",
    "#     print(label.to(torch.float32))\n",
    "#     print(label.dtype)\n",
    "#     print(out.dtype)\n",
    "    loss = F.cross_entropy(out[train_set_ind], label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,train_set_ind,test_set_ind):\n",
    "    model.eval()\n",
    "    ret = model(data.x, data.edge_index)\n",
    "    pred=ret.argmax(dim=-1)\n",
    "    \n",
    "    mask = train_set_ind\n",
    "    trainacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "    mask = test_set_ind\n",
    "    testacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "        \n",
    "    return trainacc,testacc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     dataset = CitationFull(\"./datasets\", name=\"citeseer\", transform=T.NormalizeFeatures())\n",
    "    dataset = nm\n",
    "    data = dataset[0].to(device)\n",
    "    \n",
    "    framedata=[]\n",
    "    for percent in range(1,10):\n",
    "        for num in range(10):\n",
    "\n",
    "            index=[i for i in range(dataset.x.shape[0])]\n",
    "            train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1*percent,random_state=42,stratify=dataset.y)\n",
    "\n",
    "            value=[0.1*percent,0,0,0,0]\n",
    "            \n",
    "            model = GCN(dataset.num_features, dataset.num_classes).to(device)\n",
    "            model = torch_geometric.compile(model)                   # Compile the model into an optimized version:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "            \n",
    "            for epoch in range(0, 400):\n",
    "                loss = train(model,train_set_ind,train_label_ind)\n",
    "                train_acc,test_acc = test(model,train_set_ind,test_set_ind)\n",
    "                print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train: {train_acc:.4f},'f'Test: {test_acc:.4f}')\n",
    "                if test_acc > value[4]:\n",
    "                    value[4] = test_acc \n",
    "                    value[1] = epoch\n",
    "                    value[2] = loss\n",
    "                    value[3] = train_acc\n",
    "            print(value)\n",
    "            framedata.append(value)\n",
    "            time.sleep(0.5)\n",
    "#     pd.DataFrame(framedata).to_csv(\"./GCN/GCN_{}_2L.csv\".format(dsname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2589721f",
   "metadata": {},
   "source": [
    "### GAT2L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa93aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model,train_set_ind,train_label_ind):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[train_set_ind], data.y[train_label_ind])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,train_set_ind,test_set_ind):\n",
    "    model.eval()\n",
    "    ret = model(data.x, data.edge_index)\n",
    "    pred=ret.argmax(dim=-1)\n",
    "\n",
    "    mask = train_set_ind\n",
    "    trainacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "    mask = test_set_ind\n",
    "    testacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "        \n",
    "    return trainacc,testacc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     dataset = CitationFull(\"./datasets\", name=\"citeseer\", transform=T.NormalizeFeatures())\n",
    "    dataset = nm\n",
    "    data = dataset[0].to(device)\n",
    "    \n",
    "    index=[i for i in range(dataset.x.shape[0])]\n",
    "    train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1,random_state=42,stratify=dataset.y)\n",
    "            \n",
    "\n",
    "\n",
    "    framedata=[]\n",
    "    for percent in range(1,10):\n",
    "        for num in range(10):\n",
    "\n",
    "            index=[i for i in range(dataset.x.shape[0])]\n",
    "            train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1*percent,random_state=42,stratify=dataset.y)\n",
    "            value=[0.1*percent,0,0,0,0]\n",
    "            \n",
    "                    #  GAT \n",
    "            model = GAT(in_channels=dataset.num_node_features,\n",
    "                        hidden_channels=32,\n",
    "                        num_layers=2,\n",
    "                        out_channels=dataset.num_classes).to(device)\n",
    "        #     model = torch_geometric.compile(model)                   # Compile the model into an optimized version:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "            \n",
    "            for epoch in range(0, 400):\n",
    "                pass\n",
    "                loss = train(model,train_set_ind,train_label_ind)\n",
    "                train_acc,test_acc = test(model,train_set_ind,test_set_ind)\n",
    "                print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train: {train_acc:.4f},'f'Test: {test_acc:.4f}')\n",
    "                if test_acc > value[4]:\n",
    "                    value[4] = test_acc \n",
    "                    value[1] = epoch\n",
    "                    value[2] = loss\n",
    "                    value[3] = train_acc\n",
    "            print(value)\n",
    "            framedata.append(value)\n",
    "            time.sleep(0.5)\n",
    "    pd.DataFrame(framedata).to_csv(\"./GAT/GAT_{}_2L.csv\".format(dsname))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2586501",
   "metadata": {},
   "source": [
    "### GIN-2L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff713a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model,train_set_ind,train_label_ind):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[train_set_ind], data.y[train_label_ind])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,train_set_ind,test_set_ind):\n",
    "    model.eval()\n",
    "    ret = model(data.x, data.edge_index)\n",
    "    pred=ret.argmax(dim=-1)\n",
    "\n",
    "    mask = train_set_ind\n",
    "    trainacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "    mask = test_set_ind\n",
    "    testacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "        \n",
    "    return trainacc,testacc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     dataset = CitationFull(\"./datasets\", name=\"citeseer\", transform=T.NormalizeFeatures())\n",
    "    dataset = nm\n",
    "    data = dataset[0].to(device)\n",
    "    \n",
    "\n",
    "    framedata=[]\n",
    "    for percent in range(1,10):\n",
    "        for num in range(10):\n",
    "\n",
    "            index=[i for i in range(dataset.x.shape[0])]\n",
    "            train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1*percent,random_state=42,stratify=dataset.y)\n",
    "            value=[0.1*percent,0,0,0,0]\n",
    "            \n",
    "                    #  GAT \n",
    "            model = GIN(in_channels=dataset.num_node_features,\n",
    "                        hidden_channels=32,\n",
    "                        num_layers=2,\n",
    "                        out_channels=dataset.num_classes).to(device)\n",
    "        #     model = torch_geometric.compile(model)                   # Compile the model into an optimized version:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "            \n",
    "            for epoch in range(0, 400):\n",
    "                pass\n",
    "                loss = train(model,train_set_ind,train_label_ind)\n",
    "                train_acc,test_acc = test(model,train_set_ind,test_set_ind)\n",
    "                print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train: {train_acc:.4f},'f'Test: {test_acc:.4f}')\n",
    "                if test_acc > value[4]:\n",
    "                    value[4] = test_acc \n",
    "                    value[1] = epoch\n",
    "                    value[2] = loss\n",
    "                    value[3] = train_acc\n",
    "            print(value)\n",
    "            framedata.append(value)\n",
    "            time.sleep(0.5)\n",
    "    pd.DataFrame(framedata).to_csv(\"./GIN/GIN_{}_2L.csv\".format(dsname))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cfa248",
   "metadata": {},
   "source": [
    "### GCNII-5L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7d9800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,num_heads=1):\n",
    "        super().__init__()\n",
    "#         self.fc1 = nn.Linear(in_channels,32)\n",
    "#         self.fc2=nn.Linear(64,16)\n",
    "        self.conv0 = Linear(in_channels,32)\n",
    "        self.conv1 = GCN2Conv(32,0.1)\n",
    "        self.conv2 = GCN2Conv(32,0.1)\n",
    "        self.conv3 = GCN2Conv(32,0.1)\n",
    "        self.conv4 = GCN2Conv(32,0.1)\n",
    "        self.conv5 = GCN2Conv(32,0.1)\n",
    "        self.conv6 = Linear(32,out_channels)\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.children():\n",
    "            layer.reset_parameters()   \n",
    "            \n",
    "    def forward(self, x, edge_index):\n",
    "#         print(\"conv00:{}\".format(x))\n",
    "        x0 = self.conv0(x)   ## 1hop       inf\n",
    "        x1 = self.conv1(x0,x0,edge_index).relu()\n",
    "        x2 = self.conv2(x1,x0,edge_index).relu()\n",
    "        x3 = self.conv3(x2,x0,edge_index).relu()\n",
    "        x4 = self.conv4(x3,x0,edge_index).relu()\n",
    "        x5 = self.conv5(x4,x0,edge_index).relu()\n",
    "        x6 = self.conv6(x5)\n",
    " \n",
    "        return x6      \n",
    "\n",
    "def train(model,train_set_ind,train_label_ind):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    label=torch.nn.functional.one_hot(data.y[train_label_ind], num_classes=nm.num_classes)\n",
    "    label=label.to(torch.float32)\n",
    "#     print(label.to(torch.float32))\n",
    "#     print(label.dtype)\n",
    "#     print(out.dtype)\n",
    "    loss = F.cross_entropy(out[train_set_ind], label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,train_set_ind,test_set_ind):\n",
    "    model.eval()\n",
    "    ret = model(data.x, data.edge_index)\n",
    "    pred=ret.argmax(dim=-1)\n",
    "    \n",
    "    mask = train_set_ind\n",
    "    trainacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "    mask = test_set_ind\n",
    "    testacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "        \n",
    "    return trainacc,testacc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     dataset = CitationFull(\"./datasets\", name=\"citeseer\", transform=T.NormalizeFeatures())\n",
    "    dataset = nm\n",
    "    data = dataset[0].to(device)\n",
    "    \n",
    "    framedata=[]\n",
    "    for percent in range(1,10):\n",
    "        for num in range(10):\n",
    "\n",
    "            index=[i for i in range(dataset.x.shape[0])]\n",
    "            train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1*percent,random_state=42,stratify=dataset.y)\n",
    "\n",
    "            value=[0.1*percent,0,0,0,0]\n",
    "            \n",
    "            model = GCN(dataset.num_features, dataset.num_classes).to(device)\n",
    "            model = torch_geometric.compile(model)                   # Compile the model into an optimized version:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "            \n",
    "            for epoch in range(0, 400):\n",
    "                loss = train(model,train_set_ind,train_label_ind)\n",
    "                train_acc,test_acc = test(model,train_set_ind,test_set_ind)\n",
    "                print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train: {train_acc:.4f},'f'Test: {test_acc:.4f}')\n",
    "                if test_acc > value[4]:\n",
    "                    value[4] = test_acc \n",
    "                    value[1] = epoch\n",
    "                    value[2] = loss\n",
    "                    value[3] = train_acc\n",
    "            print(value)\n",
    "            framedata.append(value)\n",
    "            time.sleep(0.5)\n",
    "    pd.DataFrame(framedata).to_csv(\"./GCNII/GCNII_{}_5L.csv\".format(dsname))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3203885f",
   "metadata": {},
   "source": [
    "#### GAT2--5l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model,train_set_ind,train_label_ind):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[train_set_ind], data.y[train_label_ind])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,train_set_ind,test_set_ind):\n",
    "    model.eval()\n",
    "    ret = model(data.x, data.edge_index)\n",
    "    pred=ret.argmax(dim=-1)\n",
    "\n",
    "    mask = train_set_ind\n",
    "    trainacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "    mask = test_set_ind\n",
    "    testacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "        \n",
    "    return trainacc,testacc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     dataset = CitationFull(\"./datasets\", name=\"citeseer\", transform=T.NormalizeFeatures())\n",
    "    dataset = nm\n",
    "    data = dataset[0].to(device)\n",
    "    \n",
    "    index=[i for i in range(dataset.x.shape[0])]\n",
    "    train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1,random_state=42,stratify=dataset.y)\n",
    "            \n",
    "\n",
    "\n",
    "    framedata=[]\n",
    "    for percent in range(1,10):\n",
    "        for num in range(10):\n",
    "\n",
    "            index=[i for i in range(dataset.x.shape[0])]\n",
    "            train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1*percent,random_state=42,stratify=dataset.y)\n",
    "            value=[0.1*percent,0,0,0,0]\n",
    "            \n",
    "                    #  GAT \n",
    "            model = GAT(in_channels=dataset.num_node_features,\n",
    "                        hidden_channels=32,\n",
    "                        num_layers=5,\n",
    "                        out_channels=dataset.num_classes,\n",
    "                        v2=True).to(device)\n",
    "        #     model = torch_geometric.compile(model)                   # Compile the model into an optimized version:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "            \n",
    "            for epoch in range(0, 400):\n",
    "                pass\n",
    "                loss = train(model,train_set_ind,train_label_ind)\n",
    "                train_acc,test_acc = test(model,train_set_ind,test_set_ind)\n",
    "                print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train: {train_acc:.4f},'f'Test: {test_acc:.4f}')\n",
    "                if test_acc > value[4]:\n",
    "                    value[4] = test_acc \n",
    "                    value[1] = epoch\n",
    "                    value[2] = loss\n",
    "                    value[3] = train_acc\n",
    "            print(value)\n",
    "            framedata.append(value)\n",
    "            time.sleep(0.5)\n",
    "    pd.DataFrame(framedata).to_csv(\"./GAT2/GAT2_{}_5L.csv\".format(dsname))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee21827",
   "metadata": {},
   "source": [
    "#### FAGCN--5L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,num_heads=1):\n",
    "        super().__init__()\n",
    "#         self.fc1 = nn.Linear(in_channels,32)\n",
    "#         self.fc2=nn.Linear(64,16)\n",
    "        self.conv0 = Linear(in_channels,32)\n",
    "        self.conv1 = FAConv(32,0.3,0.4)\n",
    "        self.conv2 = FAConv(32,0.3,0.4)\n",
    "        self.conv3 = FAConv(32,0.3,0.4)\n",
    "        self.conv4 = FAConv(32,0.3,0.4)\n",
    "        self.conv5 = FAConv(32,0.3,0.4)\n",
    "        self.conv6 = Linear(32,out_channels)\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.children():\n",
    "            layer.reset_parameters()   \n",
    "            \n",
    "    def forward(self, x, edge_index):\n",
    "#         print(\"conv00:{}\".format(x))\n",
    "        x0 = self.conv0(x)   ## 1hop       inf\n",
    "        x1 = self.conv1(x0,x0,edge_index).relu()\n",
    "        x2 = self.conv2(x1,x0,edge_index).relu()\n",
    "        x3 = self.conv3(x2,x0,edge_index).relu()\n",
    "        x4 = self.conv4(x3,x0,edge_index).relu()\n",
    "        x5 = self.conv5(x4,x0,edge_index).relu()\n",
    "        x6 = self.conv6(x5)\n",
    " \n",
    "        return x6      \n",
    "\n",
    "def train(model,train_set_ind,train_label_ind):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    label=torch.nn.functional.one_hot(data.y[train_label_ind], num_classes=nm.num_classes)\n",
    "    label=label.to(torch.float32)\n",
    "#     print(label.to(torch.float32))\n",
    "#     print(label.dtype)\n",
    "#     print(out.dtype)\n",
    "    loss = F.cross_entropy(out[train_set_ind], label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,train_set_ind,test_set_ind):\n",
    "    model.eval()\n",
    "    ret = model(data.x, data.edge_index)\n",
    "    pred=ret.argmax(dim=-1)\n",
    "    \n",
    "    mask = train_set_ind\n",
    "    trainacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "    mask = test_set_ind\n",
    "    testacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "        \n",
    "    return trainacc,testacc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     dataset = CitationFull(\"./datasets\", name=\"citeseer\", transform=T.NormalizeFeatures())\n",
    "    dataset = nm\n",
    "    data = dataset[0].to(device)\n",
    "    \n",
    "    framedata=[]\n",
    "    for percent in range(1,10):\n",
    "        for num in range(10):\n",
    "\n",
    "            index=[i for i in range(dataset.x.shape[0])]\n",
    "            train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1*percent,random_state=42,stratify=dataset.y)\n",
    "\n",
    "            value=[0.1*percent,0,0,0,0]\n",
    "            \n",
    "            model = GCN(dataset.num_features, dataset.num_classes).to(device)\n",
    "            model = torch_geometric.compile(model)                   # Compile the model into an optimized version:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "            \n",
    "            for epoch in range(0, 400):\n",
    "                loss = train(model,train_set_ind,train_label_ind)\n",
    "                train_acc,test_acc = test(model,train_set_ind,test_set_ind)\n",
    "                print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train: {train_acc:.4f},'f'Test: {test_acc:.4f}')\n",
    "                if test_acc > value[4]:\n",
    "                    value[4] = test_acc \n",
    "                    value[1] = epoch\n",
    "                    value[2] = loss\n",
    "                    value[3] = train_acc\n",
    "            print(value)\n",
    "            framedata.append(value)\n",
    "            time.sleep(0.5)\n",
    "    pd.DataFrame(framedata).to_csv(\"./FAGCN/FAGCN_{}_5L.csv\".format(dsname))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3df5b1",
   "metadata": {},
   "source": [
    "### SSGCN--5L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e37e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "#         self.fc1 = nn.Linear(in_channels,32)\n",
    "#         self.fc2=nn.Linear(64,16)\n",
    "        self.conv0 = Linear(in_channels,32)\n",
    "        self.conv1 = SSGConv(32,32,0.05)\n",
    "        self.conv2 = SSGConv(32,32,0.05)\n",
    "        self.conv3 = SSGConv(32,32,0.05)\n",
    "        self.conv4 = SSGConv(32,32,0.05)\n",
    "        self.conv5 = SSGConv(32,32,0.05)\n",
    "        self.conv6 = Linear(32,out_channels)\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.children():\n",
    "            layer.reset_parameters()   \n",
    "            \n",
    "    def forward(self, x, edge_index):\n",
    "#         print(\"conv00:{}\".format(x))\n",
    "        x0 = self.conv0(x)   ## 1hop       inf\n",
    "        x1 = self.conv1(x0,edge_index).relu()\n",
    "        x2 = self.conv2(x1,edge_index).relu()\n",
    "        x3 = self.conv3(x2,edge_index).relu()\n",
    "        x4 = self.conv4(x3,edge_index).relu()\n",
    "        x5 = self.conv5(x4,edge_index).relu()\n",
    "        x6 = self.conv6(x5)   ## 1hop       inf\n",
    " \n",
    "        return x6         \n",
    "\n",
    "def train(model,train_set_ind,train_label_ind):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    label=torch.nn.functional.one_hot(data.y[train_label_ind], num_classes=nm.num_classes)\n",
    "    label=label.to(torch.float32)\n",
    "\n",
    "    loss = F.cross_entropy(out[train_set_ind], label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,train_set_ind,test_set_ind):\n",
    "    model.eval()\n",
    "    ret = model(data.x, data.edge_index)\n",
    "    pred=ret.argmax(dim=-1)\n",
    "    \n",
    "    mask = train_set_ind\n",
    "    trainacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "    mask = test_set_ind\n",
    "    testacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "        \n",
    "    return trainacc,testacc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     dataset = CitationFull(\"./datasets\", name=\"citeseer\", transform=T.NormalizeFeatures())\n",
    "    dataset = nm\n",
    "    data = dataset[0].to(device)\n",
    "    \n",
    "    framedata=[]\n",
    "    for percent in range(1,10):\n",
    "        for num in range(10):\n",
    "\n",
    "            index=[i for i in range(dataset.x.shape[0])]\n",
    "            train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1*percent,random_state=42,stratify=dataset.y)\n",
    "\n",
    "            value=[0.1*percent,0,0,0,0]\n",
    "            \n",
    "            model = GCN(dataset.num_features, dataset.num_classes).to(device)\n",
    "            model = torch_geometric.compile(model)                   # Compile the model into an optimized version:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "            \n",
    "            for epoch in range(0, 400):\n",
    "                loss = train(model,train_set_ind,train_label_ind)\n",
    "                train_acc,test_acc = test(model,train_set_ind,test_set_ind)\n",
    "                print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train: {train_acc:.4f},'f'Test: {test_acc:.4f}')\n",
    "                if test_acc > value[4]:\n",
    "                    value[4] = test_acc \n",
    "                    value[1] = epoch\n",
    "                    value[2] = loss\n",
    "                    value[3] = train_acc\n",
    "            print(value)\n",
    "            framedata.append(value)\n",
    "            time.sleep(0.5)\n",
    "    pd.DataFrame(framedata).to_csv(\"./SSGCN/SSGCN_{}_5L.csv\".format(dsname))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19daec1",
   "metadata": {},
   "source": [
    "### generalGCN_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcdea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "#         self.fc1 = nn.Linear(in_channels,32)\n",
    "#         self.fc2=nn.Linear(64,16)\n",
    "        self.conv0 = Linear(in_channels,32)\n",
    "        self.conv1 = GeneralConv(32,32)\n",
    "        self.conv2 = GeneralConv(32,32)\n",
    "        self.conv3 = GeneralConv(32,32)\n",
    "        self.conv4 = GeneralConv(32,32)\n",
    "        self.conv5 = GeneralConv(32,32)\n",
    "        self.conv6 = Linear(32,out_channels)\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.children():\n",
    "            layer.reset_parameters()   \n",
    "            \n",
    "    def forward(self, x, edge_index):\n",
    "#         print(\"conv00:{}\".format(x))\n",
    "        x0 = self.conv0(x)   ## 1hop       inf\n",
    "        x1 = self.conv1(x0,edge_index).relu()\n",
    "        x2 = self.conv2(x1,edge_index).relu()\n",
    "        x3 = self.conv3(x2,edge_index).relu()\n",
    "        x4 = self.conv4(x3,edge_index).relu()\n",
    "        x5 = self.conv5(x4,edge_index).relu()\n",
    "        x6 = self.conv6(x5)   ## 1hop       inf\n",
    " \n",
    "        return x6      \n",
    "\n",
    "def train(model,train_set_ind,train_label_ind):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    label=torch.nn.functional.one_hot(data.y[train_label_ind], num_classes=nm.num_classes)\n",
    "    label=label.to(torch.float32)\n",
    "\n",
    "    loss = F.cross_entropy(out[train_set_ind], label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,train_set_ind,test_set_ind):\n",
    "    model.eval()\n",
    "    ret = model(data.x, data.edge_index)\n",
    "    pred=ret.argmax(dim=-1)\n",
    "    \n",
    "    mask = train_set_ind\n",
    "    trainacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "    mask = test_set_ind\n",
    "    testacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "        \n",
    "    return trainacc,testacc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     dataset = CitationFull(\"./datasets\", name=\"citeseer\", transform=T.NormalizeFeatures())\n",
    "    dataset = nm\n",
    "    data = dataset[0].to(device)\n",
    "    \n",
    "    framedata=[]\n",
    "    for percent in range(1,10):\n",
    "        for num in range(10):\n",
    "\n",
    "            index=[i for i in range(dataset.x.shape[0])]\n",
    "            train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1*percent,random_state=42,stratify=dataset.y)\n",
    "\n",
    "            value=[0.1*percent,0,0,0,0]\n",
    "            \n",
    "            model = GCN(dataset.num_features, dataset.num_classes).to(device)\n",
    "#             model = torch_geometric.compile(model)                   # Compile the model into an optimized version:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "            \n",
    "            for epoch in range(0, 400):\n",
    "                loss = train(model,train_set_ind,train_label_ind)\n",
    "                train_acc,test_acc = test(model,train_set_ind,test_set_ind)\n",
    "                print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train: {train_acc:.4f},'f'Test: {test_acc:.4f}')\n",
    "                if test_acc > value[4]:\n",
    "                    value[4] = test_acc \n",
    "                    value[1] = epoch\n",
    "                    value[2] = loss\n",
    "                    value[3] = train_acc\n",
    "            print(value)\n",
    "            framedata.append(value)\n",
    "            time.sleep(0.5)\n",
    "    pd.DataFrame(framedata).to_csv(\"./generalGCN/gGCN_{}_5L.csv\".format(dsname))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25faaa7c",
   "metadata": {},
   "source": [
    "### superGAT_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b888b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "#         self.fc1 = nn.Linear(in_channels,32)\n",
    "#         self.fc2=nn.Linear(64,16)\n",
    "        self.conv0 = Linear(in_channels,32)\n",
    "        self.conv1 = SuperGATConv(32,32)\n",
    "        self.conv2 = SuperGATConv(32,32)\n",
    "        self.conv3 = SuperGATConv(32,32)\n",
    "        self.conv4 = SuperGATConv(32,32)\n",
    "        self.conv5 = SuperGATConv(32,32)\n",
    "        self.conv6 = Linear(32,out_channels)\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.children():\n",
    "            layer.reset_parameters()   \n",
    "            \n",
    "    def forward(self, x, edge_index):\n",
    "#         print(\"conv00:{}\".format(x))\n",
    "        x0 = self.conv0(x)   ## 1hop       inf\n",
    "        x1 = self.conv1(x0,edge_index).relu()\n",
    "        x2 = self.conv2(x1,edge_index).relu()\n",
    "        x3 = self.conv3(x2,edge_index).relu()\n",
    "        x4 = self.conv4(x3,edge_index).relu()\n",
    "        x5 = self.conv5(x4,edge_index).relu()\n",
    "        x6 = self.conv6(x5)   ## 1hop       inf\n",
    " \n",
    "        return x6      \n",
    "\n",
    "def train(model,train_set_ind,train_label_ind):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    label=torch.nn.functional.one_hot(data.y[train_label_ind], num_classes=nm.num_classes)\n",
    "    label=label.to(torch.float32)\n",
    "\n",
    "    loss = F.cross_entropy(out[train_set_ind], label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,train_set_ind,test_set_ind):\n",
    "    model.eval()\n",
    "    ret = model(data.x, data.edge_index)\n",
    "    pred=ret.argmax(dim=-1)\n",
    "    \n",
    "    mask = train_set_ind\n",
    "    trainacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "    mask = test_set_ind\n",
    "    testacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "        \n",
    "    return trainacc,testacc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     dataset = CitationFull(\"./datasets\", name=\"citeseer\", transform=T.NormalizeFeatures())\n",
    "    dataset = nm\n",
    "    data = dataset[0].to(device)\n",
    "    \n",
    "    framedata=[]\n",
    "    for percent in range(1,10):\n",
    "        \n",
    "        for num in range(10):\n",
    "\n",
    "            index=[i for i in range(dataset.x.shape[0])]\n",
    "            train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1*percent,random_state=42,stratify=dataset.y)\n",
    "\n",
    "            value=[0.1*percent,0,0,0,0]\n",
    "            model = GCN(dataset.num_features, dataset.num_classes).to(device)\n",
    "        #             model = torch_geometric.compile(model)                   # Compile the model into an optimized version:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "            for epoch in range(0, 400):\n",
    "                loss = train(model,train_set_ind,train_label_ind)\n",
    "                train_acc,test_acc = test(model,train_set_ind,test_set_ind)\n",
    "                print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train: {train_acc:.4f},'f'Test: {test_acc:.4f}')\n",
    "                if test_acc > value[4]:\n",
    "                    value[4] = test_acc \n",
    "                    value[1] = epoch\n",
    "                    value[2] = loss\n",
    "                    value[3] = train_acc\n",
    "            print(value)\n",
    "            framedata.append(value)\n",
    "            time.sleep(0.5)\n",
    "            del model\n",
    "            del optimizer\n",
    "    pd.DataFrame(framedata).to_csv(\"./superGAT/superGAT_{}_5L.csv\".format(dsname))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00c0ebf",
   "metadata": {},
   "source": [
    "### GEN5L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "#         self.fc1 = nn.Linear(in_channels,32)\n",
    "#         self.fc2=nn.Linear(64,16)\n",
    "        self.conv0 = Linear(in_channels,32)\n",
    "        self.conv1 = GENConv(32,32)\n",
    "        self.conv2 = GENConv(32,32)\n",
    "        self.conv3 = GENConv(32,32)\n",
    "        self.conv4 = GENConv(32,32)\n",
    "        self.conv5 = GENConv(32,32)\n",
    "        self.conv6 = Linear(32,out_channels)\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.children():\n",
    "            layer.reset_parameters()   \n",
    "            \n",
    "    def forward(self, x, edge_index):\n",
    "#         print(\"conv00:{}\".format(x))\n",
    "        x0 = self.conv0(x)   ## 1hop       inf\n",
    "        x1 = self.conv1(x0,edge_index).relu()\n",
    "        x2 = self.conv2(x1,edge_index).relu()\n",
    "        x3 = self.conv3(x2,edge_index).relu()\n",
    "        x4 = self.conv4(x3,edge_index).relu()\n",
    "        x5 = self.conv5(x4,edge_index).relu()\n",
    "        x6 = self.conv6(x5)   ## 1hop       inf\n",
    " \n",
    "        return x6      \n",
    "\n",
    "def train(model,train_set_ind,train_label_ind):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    label=torch.nn.functional.one_hot(data.y[train_label_ind], num_classes=nm.num_classes)\n",
    "    label=label.to(torch.float32)\n",
    "\n",
    "    loss = F.cross_entropy(out[train_set_ind], label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,train_set_ind,test_set_ind):\n",
    "    model.eval()\n",
    "    ret = model(data.x, data.edge_index)\n",
    "    pred=ret.argmax(dim=-1)\n",
    "    \n",
    "    mask = train_set_ind\n",
    "    trainacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "    mask = test_set_ind\n",
    "    testacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "        \n",
    "    return trainacc,testacc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     dataset = CitationFull(\"./datasets\", name=\"citeseer\", transform=T.NormalizeFeatures())\n",
    "    dataset = nm\n",
    "    data = dataset[0].to(device)\n",
    "    \n",
    "    framedata=[]\n",
    "    for percent in range(1,10):\n",
    "        for num in range(10):\n",
    "\n",
    "            index=[i for i in range(dataset.x.shape[0])]\n",
    "            train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1*percent,random_state=42,stratify=dataset.y)\n",
    "\n",
    "            value=[0.1*percent,0,0,0,0]\n",
    "            \n",
    "            model = GCN(dataset.num_features, dataset.num_classes).to(device)\n",
    "#             model = torch_geometric.compile(model)                   # Compile the model into an optimized version:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "            \n",
    "            for epoch in range(0, 400):\n",
    "                loss = train(model,train_set_ind,train_label_ind)\n",
    "                train_acc,test_acc = test(model,train_set_ind,test_set_ind)\n",
    "                print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train: {train_acc:.4f},'f'Test: {test_acc:.4f}')\n",
    "                if test_acc > value[4]:\n",
    "                    value[4] = test_acc \n",
    "                    value[1] = epoch\n",
    "                    value[2] = loss\n",
    "                    value[3] = train_acc\n",
    "            print(value)\n",
    "            framedata.append(value)\n",
    "            time.sleep(0.5)\n",
    "    pd.DataFrame(framedata).to_csv(\"./GEN/GEN_{}_5L.csv\".format(dsname))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859a80a2",
   "metadata": {},
   "source": [
    "#### PMLP5L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79784da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model,train_set_ind,train_label_ind):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[train_set_ind], data.y[train_label_ind])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,train_set_ind,test_set_ind):\n",
    "    model.eval()\n",
    "    ret = model(data.x, data.edge_index)\n",
    "    pred=ret.argmax(dim=-1)\n",
    "\n",
    "    mask = train_set_ind\n",
    "    trainacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "    mask = test_set_ind\n",
    "    testacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "        \n",
    "    return trainacc,testacc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     dataset = CitationFull(\"./datasets\", name=\"citeseer\", transform=T.NormalizeFeatures())\n",
    "    dataset = nm\n",
    "    data = dataset[0].to(device)\n",
    "    \n",
    "    index=[i for i in range(dataset.x.shape[0])]\n",
    "    train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1,random_state=42,stratify=dataset.y)\n",
    "            \n",
    "\n",
    "\n",
    "    framedata=[]\n",
    "    for percent in range(1,10):\n",
    "        for num in range(10):\n",
    "\n",
    "            index=[i for i in range(dataset.x.shape[0])]\n",
    "            train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1*percent,random_state=42,stratify=dataset.y)\n",
    "            value=[0.1*percent,0,0,0,0]\n",
    "                                    \n",
    "            model = PMLP(in_channels=dataset.num_node_features,\n",
    "                        hidden_channels=32,\n",
    "                        out_channels=dataset.num_classes,\n",
    "                        num_layers=5).to(device)                                    \n",
    "                                                 \n",
    "        #     model = torch_geometric.compile(model)                   # Compile the model into an optimized version:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "            \n",
    "            for epoch in range(0, 400):\n",
    "                pass\n",
    "                loss = train(model,train_set_ind,train_label_ind)\n",
    "                train_acc,test_acc = test(model,train_set_ind,test_set_ind)\n",
    "                print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train: {train_acc:.4f},'f'Test: {test_acc:.4f}')\n",
    "                if test_acc > value[4]:\n",
    "                    value[4] = test_acc \n",
    "                    value[1] = epoch\n",
    "                    value[2] = loss\n",
    "                    value[3] = train_acc\n",
    "            print(value)\n",
    "            framedata.append(value)\n",
    "            time.sleep(0.5)\n",
    "    pd.DataFrame(framedata).to_csv(\"./PMLP/PMLP_{}_5L.csv\".format(dsname))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3eb0f",
   "metadata": {},
   "source": [
    "### SG--5L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f37342a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.6474, Train: 0.1464,Test: 0.1467\n",
      "Epoch: 1, Loss: 1.6274, Train: 0.1492,Test: 0.1499\n",
      "Epoch: 2, Loss: 1.6082, Train: 0.3823,Test: 0.3870\n",
      "Epoch: 3, Loss: 1.5874, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 4, Loss: 1.5632, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 5, Loss: 1.5335, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 6, Loss: 1.4941, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 7, Loss: 1.4386, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 8, Loss: 1.3634, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 9, Loss: 1.2777, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 10, Loss: 1.2156, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 11, Loss: 1.2149, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 12, Loss: 1.2043, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 13, Loss: 1.1496, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 14, Loss: 1.0938, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 15, Loss: 1.0622, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 16, Loss: 1.0474, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 17, Loss: 1.0344, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 18, Loss: 1.0167, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 19, Loss: 0.9952, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 20, Loss: 0.9746, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 21, Loss: 0.9589, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 22, Loss: 0.9484, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 23, Loss: 0.9400, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 24, Loss: 0.9294, Train: 0.5052,Test: 0.5052\n",
      "Epoch: 25, Loss: 0.9157, Train: 0.5057,Test: 0.5052\n",
      "Epoch: 26, Loss: 0.9012, Train: 0.5787,Test: 0.5806\n",
      "Epoch: 27, Loss: 0.8897, Train: 0.6490,Test: 0.6501\n",
      "Epoch: 28, Loss: 0.8815, Train: 0.6568,Test: 0.6577\n",
      "Epoch: 29, Loss: 0.8714, Train: 0.6594,Test: 0.6594\n",
      "Epoch: 30, Loss: 0.8568, Train: 0.6611,Test: 0.6594\n",
      "Epoch: 31, Loss: 0.8393, Train: 0.6617,Test: 0.6609\n",
      "Epoch: 32, Loss: 0.8208, Train: 0.6623,Test: 0.6623\n",
      "Epoch: 33, Loss: 0.8012, Train: 0.6627,Test: 0.6623\n",
      "Epoch: 34, Loss: 0.7821, Train: 0.6624,Test: 0.6638\n",
      "Epoch: 35, Loss: 0.7670, Train: 0.6621,Test: 0.6626\n",
      "Epoch: 36, Loss: 0.7606, Train: 0.6632,Test: 0.6629\n",
      "Epoch: 37, Loss: 0.7550, Train: 0.6668,Test: 0.6658\n",
      "Epoch: 38, Loss: 0.7372, Train: 0.6701,Test: 0.6687\n",
      "Epoch: 39, Loss: 0.7142, Train: 0.6749,Test: 0.6742\n",
      "Epoch: 40, Loss: 0.6953, Train: 0.6811,Test: 0.6777\n",
      "Epoch: 41, Loss: 0.6819, Train: 0.6853,Test: 0.6812\n",
      "Epoch: 42, Loss: 0.6712, Train: 0.6896,Test: 0.6875\n",
      "Epoch: 43, Loss: 0.6620, Train: 0.6936,Test: 0.6881\n",
      "Epoch: 44, Loss: 0.6507, Train: 0.6973,Test: 0.6928\n",
      "Epoch: 45, Loss: 0.6341, Train: 0.7011,Test: 0.7012\n",
      "Epoch: 46, Loss: 0.6206, Train: 0.6957,Test: 0.6968\n",
      "Epoch: 47, Loss: 0.6113, Train: 0.6917,Test: 0.6933\n",
      "Epoch: 48, Loss: 0.6031, Train: 0.6938,Test: 0.6959\n",
      "Epoch: 49, Loss: 0.5988, Train: 0.7002,Test: 0.7035\n",
      "Epoch: 50, Loss: 0.5914, Train: 0.7061,Test: 0.7145\n",
      "Epoch: 51, Loss: 0.5865, Train: 0.7176,Test: 0.7293\n",
      "Epoch: 52, Loss: 0.5824, Train: 0.7410,Test: 0.7481\n",
      "Epoch: 53, Loss: 0.5737, Train: 0.7440,Test: 0.7519\n",
      "Epoch: 54, Loss: 0.5682, Train: 0.7478,Test: 0.7565\n",
      "Epoch: 55, Loss: 0.5621, Train: 0.7532,Test: 0.7606\n",
      "Epoch: 56, Loss: 0.5576, Train: 0.7606,Test: 0.7635\n",
      "Epoch: 57, Loss: 0.5541, Train: 0.7624,Test: 0.7684\n",
      "Epoch: 58, Loss: 0.5491, Train: 0.7621,Test: 0.7678\n",
      "Epoch: 59, Loss: 0.5429, Train: 0.7628,Test: 0.7687\n",
      "Epoch: 60, Loss: 0.5369, Train: 0.7661,Test: 0.7701\n",
      "Epoch: 61, Loss: 0.5306, Train: 0.7692,Test: 0.7736\n",
      "Epoch: 62, Loss: 0.5253, Train: 0.7705,Test: 0.7745\n",
      "Epoch: 63, Loss: 0.5189, Train: 0.7718,Test: 0.7754\n",
      "Epoch: 64, Loss: 0.5130, Train: 0.7744,Test: 0.7762\n",
      "Epoch: 65, Loss: 0.5058, Train: 0.7789,Test: 0.7800\n",
      "Epoch: 66, Loss: 0.5001, Train: 0.7785,Test: 0.7780\n",
      "Epoch: 67, Loss: 0.4961, Train: 0.7772,Test: 0.7788\n",
      "Epoch: 68, Loss: 0.5083, Train: 0.7758,Test: 0.7783\n",
      "Epoch: 69, Loss: 0.5302, Train: 0.7816,Test: 0.7820\n",
      "Epoch: 70, Loss: 0.4803, Train: 0.7709,Test: 0.7725\n",
      "Epoch: 71, Loss: 0.5099, Train: 0.8054,Test: 0.8078\n",
      "Epoch: 72, Loss: 0.4712, Train: 0.7969,Test: 0.8003\n",
      "Epoch: 73, Loss: 0.4776, Train: 0.7930,Test: 0.7919\n",
      "Epoch: 74, Loss: 0.4616, Train: 0.7951,Test: 0.7974\n",
      "Epoch: 75, Loss: 0.4571, Train: 0.8165,Test: 0.8177\n",
      "Epoch: 76, Loss: 0.4417, Train: 0.8335,Test: 0.8310\n",
      "Epoch: 77, Loss: 0.4423, Train: 0.8309,Test: 0.8278\n",
      "Epoch: 78, Loss: 0.4168, Train: 0.8110,Test: 0.8130\n",
      "Epoch: 79, Loss: 0.4197, Train: 0.8412,Test: 0.8380\n",
      "Epoch: 80, Loss: 0.3974, Train: 0.8802,Test: 0.8797\n",
      "Epoch: 81, Loss: 0.3922, Train: 0.8870,Test: 0.8855\n",
      "Epoch: 82, Loss: 0.3771, Train: 0.8869,Test: 0.8870\n",
      "Epoch: 83, Loss: 0.3647, Train: 0.8944,Test: 0.9009\n",
      "Epoch: 84, Loss: 0.3528, Train: 0.9014,Test: 0.9070\n",
      "Epoch: 85, Loss: 0.3384, Train: 0.8974,Test: 0.8988\n",
      "Epoch: 86, Loss: 0.3318, Train: 0.9049,Test: 0.9107\n",
      "Epoch: 87, Loss: 0.3164, Train: 0.9079,Test: 0.9125\n",
      "Epoch: 88, Loss: 0.3089, Train: 0.9114,Test: 0.9180\n",
      "Epoch: 89, Loss: 0.3010, Train: 0.9121,Test: 0.9186\n",
      "Epoch: 90, Loss: 0.2868, Train: 0.9119,Test: 0.9188\n",
      "Epoch: 91, Loss: 0.2816, Train: 0.9173,Test: 0.9238\n",
      "Epoch: 92, Loss: 0.2730, Train: 0.9198,Test: 0.9249\n",
      "Epoch: 93, Loss: 0.2645, Train: 0.9210,Test: 0.9272\n",
      "Epoch: 94, Loss: 0.2556, Train: 0.9212,Test: 0.9284\n",
      "Epoch: 95, Loss: 0.2525, Train: 0.9257,Test: 0.9322\n",
      "Epoch: 96, Loss: 0.2427, Train: 0.9275,Test: 0.9339\n",
      "Epoch: 97, Loss: 0.2400, Train: 0.9274,Test: 0.9322\n",
      "Epoch: 98, Loss: 0.2336, Train: 0.9270,Test: 0.9313\n",
      "Epoch: 99, Loss: 0.2301, Train: 0.9310,Test: 0.9374\n",
      "Epoch: 100, Loss: 0.2225, Train: 0.9329,Test: 0.9380\n",
      "Epoch: 101, Loss: 0.2207, Train: 0.9321,Test: 0.9400\n"
     ]
    }
   ],
   "source": [
    "class SG(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "#         self.fc1 = nn.Linear(in_channels,32)\n",
    "#         self.fc2=nn.Linear(64,16)\n",
    "        self.conv0 = Linear(in_channels,32)\n",
    "        self.conv1 = SGConv(32,32)\n",
    "        self.conv2 = SGConv(32,32)\n",
    "        self.conv3 = SGConv(32,32)\n",
    "        self.conv4 = SGConv(32,32)\n",
    "        self.conv5 = SGConv(32,32)\n",
    "        self.conv6 = Linear(32,out_channels)\n",
    "    def reset_parameters(self):\n",
    "        for layer in self.children():\n",
    "            layer.reset_parameters()   \n",
    "            \n",
    "    def forward(self, x, edge_index):\n",
    "#         print(\"conv00:{}\".format(x))\n",
    "        x0 = self.conv0(x)   ## 1hop       inf\n",
    "        x1 = self.conv1(x0,edge_index).relu()\n",
    "        x2 = self.conv2(x1,edge_index).relu()\n",
    "        x3 = self.conv3(x2,edge_index).relu()\n",
    "        x4 = self.conv4(x3,edge_index).relu()\n",
    "        x5 = self.conv5(x4,edge_index).relu()\n",
    "        x6 = self.conv6(x5)   ## 1hop       inf\n",
    "        return x6  \n",
    "    \n",
    "    \n",
    "def train(model,train_set_ind,train_label_ind):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    label=torch.nn.functional.one_hot(data.y[train_label_ind], num_classes=nm.num_classes)\n",
    "    label=label.to(torch.float32)\n",
    "\n",
    "    loss = F.cross_entropy(out[train_set_ind], label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,train_set_ind,test_set_ind):\n",
    "    model.eval()\n",
    "    ret = model(data.x, data.edge_index)\n",
    "    pred=ret.argmax(dim=-1)\n",
    "    \n",
    "    mask = train_set_ind\n",
    "    trainacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "    mask = test_set_ind\n",
    "    testacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "        \n",
    "    return trainacc,testacc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     dataset = CitationFull(\"./datasets\", name=\"citeseer\", transform=T.NormalizeFeatures())\n",
    "    dataset = nm\n",
    "    data = dataset[0].to(device)\n",
    "    \n",
    "    framedata=[]\n",
    "    for percent in range(1,10):\n",
    "        for num in range(10):\n",
    "\n",
    "            index=[i for i in range(dataset.x.shape[0])]\n",
    "            train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1*percent,random_state=42,stratify=dataset.y)\n",
    "\n",
    "            value=[0.1*percent,0,0,0,0]\n",
    "            \n",
    "            model = SG(dataset.num_features, dataset.num_classes).to(device)\n",
    "#             model = torch_geometric.compile(model)                   # Compile the model into an optimized version:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "            \n",
    "            for epoch in range(0, 400):\n",
    "                loss = train(model,train_set_ind,train_label_ind)\n",
    "                train_acc,test_acc = test(model,train_set_ind,test_set_ind)\n",
    "                print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train: {train_acc:.4f},'f'Test: {test_acc:.4f}')\n",
    "                if test_acc > value[4]:\n",
    "                    value[4] = test_acc \n",
    "                    value[1] = epoch\n",
    "                    value[2] = loss\n",
    "                    value[3] = train_acc\n",
    "            print(value)\n",
    "            framedata.append(value)\n",
    "            time.sleep(0.5)\n",
    "    pd.DataFrame(framedata).to_csv(\"./SG/SG_{}_5L.csv\".format(dsname))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0bf277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d1fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "726e3852",
   "metadata": {},
   "source": [
    "### SLD-9L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mynorm(tensor):\n",
    "    # \n",
    "    min_values = tensor.min(dim=1, keepdim=True)[0]\n",
    "    max_values = tensor.max(dim=1, keepdim=True)[0]\n",
    "    # -\n",
    "#     normalized_tensor = (tensor - min_values) / (max_values - min_values + 1e-8)               ### 0-1\n",
    "    normalized_tensor = 2 * (tensor - min_values) / (max_values - min_values + 1e-8) - 1       ### -1-1\n",
    "    return normalized_tensor\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_channels,32)\n",
    "#         self.fc2=nn.Linear(64,16)\n",
    "        self.conv1 = GCNConv(32,32)\n",
    "        self.conv2 = GCNConv(32,32)\n",
    "        self.conv3 = GCNConv(32,32)\n",
    "        self.conv4 = GCNConv(32,32)\n",
    "        self.conv5 = GCNConv(32,32)\n",
    "        self.conv6 = GCNConv(32,32)\n",
    "        self.conv7 = GCNConv(32,32)\n",
    "        self.conv8 = GCNConv(32,32)\n",
    "        \n",
    "        \n",
    "#         self.conv7 = GCNConv(16*6,out_channels)\n",
    "        self.conv9 = nn.Linear(32*9,out_channels)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for layer in self.children():\n",
    "            layer.reset_parameters()    \n",
    "    \n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "#         print(\"conv00:{}\".format(x))\n",
    "        x0= self.fc1(x).relu()\n",
    "        x0=mynorm(x0)\n",
    "        x1 = self.conv1(x0, edge_index)         ## 1hop       inf\n",
    "        xx1= x1\n",
    "        x2 = self.conv2(x1, edge_index)         ## 0,2hop       inf\n",
    "        xx2=mynorm(x2)- mynorm(x0)\n",
    "        x3 = self.conv3(x2, edge_index)          ## 1,3hop     inf\n",
    "        xx3=mynorm(x3)-mynorm(x1)\n",
    "        \n",
    "        x4 = self.conv4(x3, edge_index)         ## 0,2,4hop   inf\n",
    "        xx4=mynorm(x4)-mynorm(x2) \n",
    "        \n",
    "        x5 = self.conv5(x4, edge_index)          ## 1,3,5hop   inf\n",
    "        xx5=mynorm(x5)-mynorm(x3)\n",
    "        \n",
    "        x6 = self.conv6(x5, edge_index)         ## 0,2,4,6hop inf\n",
    "        xx6=mynorm(x6)-mynorm(x4)\n",
    "\n",
    "        \n",
    "        x7 = self.conv7(x6, edge_index)          ## 1,3,5,7 hop   inf\n",
    "        xx7=mynorm(x7)-mynorm(x5)\n",
    "        \n",
    "        x8 = self.conv8(x7, edge_index)         ## 0,2,4,6,8 hop inf\n",
    "        xx8=mynorm(x8)-mynorm(x6)\n",
    "        \n",
    "        \n",
    "        xx9=torch.cat((x0,xx1,xx2,xx3,xx4,xx5,xx6,xx7,xx8),dim=1)\n",
    "        x9 = self.conv9(xx9) \n",
    "        return x9       \n",
    "\n",
    "\n",
    "def train(model,train_set_ind,train_label_ind):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[train_set_ind], data.y[train_label_ind])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model,train_set_ind,test_set_ind):\n",
    "    model.eval()\n",
    "    ret = model(data.x, data.edge_index)\n",
    "    pred=ret.argmax(dim=-1)\n",
    "\n",
    "    mask = train_set_ind\n",
    "    trainacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "    mask = test_set_ind\n",
    "    testacc=int((pred[mask] == data.y[mask]).sum()) / int(len(mask))\n",
    "        \n",
    "    return trainacc,testacc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     dataset = CitationFull(\"./datasets\", name=\"citeseer\", transform=T.NormalizeFeatures())\n",
    "    dataset = nm\n",
    "    data = dataset[0].to(device)\n",
    "    \n",
    "\n",
    "    framedata=[]\n",
    "    for percent in range(1,10):\n",
    "        for num in range(10):\n",
    "\n",
    "            index=[i for i in range(dataset.x.shape[0])]\n",
    "            train_set_ind,test_set_ind,train_label_ind,test_label_ind=train_test_split(index,index,test_size=0.1*percent,random_state=42,stratify=dataset.y)\n",
    "            value=[0.1*percent,0,0,0,0]\n",
    "            \n",
    "            model = GCN(dataset.num_features, dataset.num_classes).to(device)\n",
    "            model = torch_geometric.compile(model)                   # Compile the model into an optimized version:\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "            \n",
    "            \n",
    "            for epoch in range(0, 400):\n",
    "                loss = train(model,train_set_ind,train_label_ind)\n",
    "                    # \n",
    "\n",
    "                train_acc,test_acc = test(model,train_set_ind,test_set_ind)\n",
    "                print(f'Epoch: {epoch}, Loss: {loss:.4f}, Train: {train_acc:.4f},'f'Test: {test_acc:.4f}')\n",
    "                if test_acc > value[4]:\n",
    "                    value[4] = test_acc \n",
    "                    value[1] = epoch\n",
    "                    value[2] = loss\n",
    "                    value[3] = train_acc\n",
    "            print(value)\n",
    "            framedata.append(value)\n",
    "            time.sleep(0.5)\n",
    "    pd.DataFrame(framedata).to_csv(\"CS_9L.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372f74a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
